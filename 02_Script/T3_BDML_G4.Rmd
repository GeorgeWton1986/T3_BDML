---
title: "Taller 3"
author: LAURA SARIF RIVERA SANABRIA,   NICOLAS JACOME VELASCO, JORGE ELIECER VIAFARA
  MORALES, ZAIRA ALEJANDRA GARCIA BERNAL
date: "2025-05-03"
output: html_document
---

# 1. Definición de entorno de trabajo

```{r}
# Instalar y cargar el paquetes
if (!require("pacman")) install.packages("pacman")
library(pacman)

if (!require("MLmetrics")) install.packages("MLmetrics")
library(MLmetrics)

# Usar pacman para cargar (e instalar si es necesario) los paquetes
p_load(tidyverse,   # Manipulación y visualización de datos 
       dplyr,       # Manipulación de datos
       ggplot2,     # Visualización de datos en gráficas
       readr,       # Importación de datos
       stargazer,   # Formato para tablas
       utils,       # Lctura de archivos y manipulación de datos
       skimr,       # Resumen estadístico 
       caret,       # Creación y validación de modelos predictivos 
       glmnet,      # Modelos de regresión penalizados (Ridge, Lasso)
       xgboost,     # Implementación de Gradient Boosting
       rpart,       # Árboles de decisión
       rpart.plot,  # Visualización de árboles generados por rpart
       pROC,        # Curvas ROC y métricas 
       Metrics,     # Métricas de evaluación
       httr,        # Solicitudes http
       rio,         # Facilidad para importar data
       plotly,      # Gráficos interactivos
       osmdata,     # Obtener información de open street maps
       sf,          # Leer/escribir/manipular datos espaciales
       leaflet,     # Mapas interactivos
       gridExtra,   # Graficar en grid
       tmaptools,   # Geocode_OMS()
       geosphere,   # Calcular distancias geográficas como Haversine
       tidymodels,  # Carga recipes, parsnip, rsample, tune, y otros paquetes relacionados
       spatialsample, # Muestreo espacial
       )
```

## 1.1 Carga base de datos

```{r}
# 1. Carga de datos train 
## Definir URL del archivo Excel en GitHub
url_excel_train <- "https://raw.githubusercontent.com/GeorgeWton1986/T3_BDML/refs/heads/main/03_Stores/train.csv"

## Descargar el archivo temporalmente
temp_file <- tempfile(fileext = ".csv")
GET(url_excel_train, write_disk(temp_file, overwrite = TRUE))

## Leer el archivo CSV en un dataframe
train_aptos <- read_csv(temp_file)


# 2. Carga de datos test hogares
## Definir URL del archivo Excel en GitHub
url_excel_test <- "https://raw.githubusercontent.com/GeorgeWton1986/T3_BDML/refs/heads/main/03_Stores/test.csv"

## Descargar el archivo temporalmente
temp_file <- tempfile(fileext = ".csv")
GET(url_excel_test, write_disk(temp_file, overwrite = TRUE))

# Leer el archivo CSV en un dataframe
test_aptos <- read_csv(temp_file)
```

## 1.2 Inspección y tratamiento BD train

```{r}
# Nombre de las columnas de la base train_hogares
colnames(train_aptos)

# Seleccion de la columnas id de hogares
train_aptos %>%
  select(property_id) %>%
  head()

# Resumen de apartamentos de la base train_hogares
skim(train_aptos)

# Tratamiento de missing values
train_miss <- skim(train_aptos) %>%
  filter(skim_type == "numeric") %>%
  select(skim_variable, n_missing)  

nobs_train <- nrow(train_aptos)

# Porcentaje de missing por cada caracteristica
train_aptos_miss<- train_miss %>% 
  mutate(p_missing= n_missing/nobs_train) %>% 
  filter(n_missing!= 0) %>% 
  arrange(-n_missing)

train_aptos_miss

# Visualizar variables con missing values

## Gráfico para surface_total (área total)
graf_ms1 <- ggplot(train_aptos, aes(surface_total)) +
  geom_histogram(color = "#000000", fill = "#0099F8") +
  geom_vline(xintercept = median(train_aptos$surface_total, na.rm = TRUE), linetype = "dashed", color = "red") +
  geom_vline(xintercept = mean(train_aptos$surface_total, na.rm = TRUE), linetype = "dashed", color = "blue") + 
  ggtitle("GRÁFICA 1 - Área total") +
  theme_bw() +
  theme(plot.title = element_text(size = 18))

ggsave("../04_Views/GRAFICO1-HISTOGRAMA-AREATOTAL.png", plot = graf_ms1, dpi = 150, width = 8, height = 6)

## Gráfico para surface_covered (área privada)
graf_ms2 <- ggplot(train_aptos, aes(surface_covered)) +
  geom_histogram(color = "#000000", fill = "#0099F8") +
  geom_vline(xintercept = median(train_aptos$surface_covered, na.rm = TRUE), linetype = "dashed", color = "red") +
  geom_vline(xintercept = mean(train_aptos$surface_covered, na.rm = TRUE), linetype = "dashed", color = "blue") + 
  ggtitle("GRÁFICA 2 - Área privada") +
  theme_bw() +
  theme(plot.title = element_text(size = 18))

ggsave("../04_Views/GRAFICO2-HISTOGRAMA-AREAPRIVADA.png", plot = graf_ms2, dpi = 150, width = 8, height = 6)

## Gráfico para rooms (habitaciones)
graf_ms3 <- ggplot(train_aptos, aes(rooms)) +
  geom_histogram(color = "#000000", fill = "#0099F8") +
  geom_vline(xintercept = median(train_aptos$rooms, na.rm = TRUE), linetype = "dashed", color = "red") +
  geom_vline(xintercept = mean(train_aptos$rooms, na.rm = TRUE), linetype = "dashed", color = "blue") + 
  ggtitle("GRÁFICA 3 -Habitaciones") +
  theme_bw() +
  theme(plot.title = element_text(size = 18))

ggsave("../04_Views/GRAFICO3-HISTOGRAMA-HABITACIONES.png", plot = graf_ms3, dpi = 150, width = 8, height = 6)

## Gráfico para bathrooms (baños)
graf_ms4 <- ggplot(train_aptos, aes(bathrooms)) +
  geom_histogram(color = "#000000", fill = "#0099F8") +
  geom_vline(xintercept = median(train_aptos$bathrooms, na.rm = TRUE), linetype = "dashed", color = "red") +
  geom_vline(xintercept = mean(train_aptos$bathrooms, na.rm = TRUE), linetype = "dashed", color = "blue") + 
  ggtitle("GRÁFICA 4 - Baños") +
  theme_bw() +
  theme(plot.title = element_text(size = 18))

ggsave("../04_Views/GRAFICO4-HISTOGRAMA-BAÑOS.png", plot = graf_ms4, dpi = 150, width = 8, height = 6)

# Imputación de variables con la mediana y moda

## 1. Variable surface_total con mediana
mediana_surface_total <- median(train_aptos$surface_total, na.rm = TRUE)

train_aptos <- train_aptos  %>%
  mutate(surface_total = ifelse(is.na(surface_total) == TRUE, mediana_surface_total, surface_total))

## 2. Variable surface_covered con mediana
mediana_surface_covered <- median(train_aptos$surface_covered, na.rm = TRUE)

train_aptos <- train_aptos  %>%
  mutate(surface_covered = ifelse(is.na(surface_covered) == TRUE, mediana_surface_covered, surface_covered))

## 3.  Variable rooms con la moda
moda_rooms <- train_aptos %>% 
  filter(!is.na(rooms)) %>% 
  count(rooms) %>% 
  arrange(desc(n)) %>% 
  slice(1) %>% 
  pull(rooms)

train_aptos <- train_aptos  %>%
  mutate(rooms = ifelse(is.na(rooms) == TRUE, moda_rooms, rooms))

## 4.Variable bathrooms con la moda
moda_bathrooms <- train_aptos %>% 
  filter(!is.na(bathrooms)) %>% 
  count(bathrooms) %>% 
  arrange(desc(n)) %>% 
  slice(1) %>% 
  pull(bathrooms)

train_aptos <- train_aptos  %>%
  mutate(bathrooms = ifelse(is.na(bathrooms) == TRUE, moda_bathrooms, bathrooms))

# Validación de correcta imputación de missing values en variables numéricas
skim(train_aptos)
```

Explorar la base para identificar outliers o anomalias de los datos

```{r}
train_aptos <- as.data.frame(train_aptos)
stargazer(train_aptos, type = "text")

# Calcular el precio por metro cuadrado
train_aptos <- train_aptos %>%
  mutate(precio_mt2 = round(price / surface_total, 0))

skim(train_aptos)

# Histograma del precio por metro cuadrado
graf_5 <- ggplot(train_aptos, aes(precio_mt2)) +
  geom_histogram(color = "#000000", fill = "#0099F8") +
  ggtitle("GRÁFICA 4 - Precio por metro cuadrado (millones)") +
  scale_x_continuous(labels = function(x) x / 1e6) +
  theme_bw() +
  theme(plot.title = element_text(size = 18))

ggsave("../04_Views/GRAFICO5-HISTOGRAMA-PRECIOXMETROCUADRADO.png", plot = graf_5, dpi = 150, width = 8, height = 6)

# Box plot del precio por metro cuadrado
graf_6 <- train_aptos %>%
  ggplot(aes(y = precio_mt2)) +
  geom_boxplot(fill = "darkblue", alpha = 0.4) +
  labs(
    title = "Muestra completa",
    y = "Precio por metro cuadrado (millones)", x = "") +
  scale_y_continuous(labels = function(x) x / 1e6) +
  theme_bw()

ggsave("../04_Views/GRAFICO6-BOXPLOT-PRECIOXMETROCUADRADO.png", plot = graf_6, dpi = 150, width = 8, height = 6)

# Ante outliers superiores se aplica la Metodología de percentil 99%
#p1 <- quantile(train_aptos$precio_mt2, probs = 0.01, na.rm = TRUE)
p99 <- quantile(train_aptos$precio_mt2, probs = 0.99, na.rm = TRUE)

train_aptos <- train_aptos %>%
  filter(precio_mt2 <= p99)

# Filtrar solo los valores menores o iguales al límite superior
graf_7 <- train_aptos %>%
  ggplot(aes(y = precio_mt2)) +
  geom_boxplot(fill = "darkblue", alpha = 0.4) +
  labs(
    title = "Muestra filtrada - P99",
    y = "Precio por metro cuadrado (millones)", x = "") +
  scale_y_continuous(labels = function(x) x / 1e6) +
  theme_bw()

ggsave("../04_Views/GRAFICO6-BOXPLOT-PRECIOXMETROCUADRADO-P99.png", plot = graf_7, dpi = 150, width = 8, height = 6)

```

Identificar en el mapa la ubicacion de los apartamentos y casas

```{r}
# Observamos la primera visualización de la ubicación de los inmubles de la base train_aptos
leaflet() %>%
  addTiles() %>%
  addCircles(lng = train_aptos$lon, 
             lat = train_aptos$lat)
```

```{r}
range_lon <- range(train_aptos$lon, na.rm = TRUE)
range_lat <- range(train_aptos$lat, na.rm = TRUE)
```

Carga de datos espaciales Identificar localidades sin Chapinero

```{r}
# Columas númericas
train_aptos <- train_aptos %>%
  mutate(lon = as.numeric(lon), lat = as.numeric(lat))

limite_chapinero <- getbb("Chapinero, Bogotá, Colombia")


# Las coordenadas en EPSG:4326
train_aptos_sf <- st_as_sf(train_aptos, 
                           coords = c("lon", "lat"), 
                           crs = 4326)

# Filtrar límites sin chapinero
train_localidades <- train_aptos %>%
  filter(
    lon < limite_chapinero[1, "min"] | lon > limite_chapinero[1, "max"] |
    lat < limite_chapinero[2, "min"] | lat > limite_chapinero[2, "max"]
  )

#Train_localicadades NO incluye chapinero
train_localidades_sf <- st_as_sf(train_localidades, 
                                 coords = c("lon", "lat"), 
                                 crs = 4326)

# Visualizar
leaflet() %>%
  addTiles() %>%
  addCircles(lng = train_localidades$lon, 
             lat = train_localidades$lat)
```

```{r}
library(ggplot2)

# Mapa formato plano

ggplot(train_localidades_sf) +
  geom_sf(color = "blue", size = 1.5, alpha = 0.7) +
  theme_minimal() +
  labs(title = "Puntos fuera de Chapinero en Bogotá",
       x = "Longitud", y = "Latitud")

# Mapar formato plano

ggplot(train_aptos_sf) +   # Aquí usas el objeto sf
  geom_sf(color = "blue", alpha = 0.6, size = 1.2) +
  coord_sf(
    xlim = c(min(train_aptos$lon), max(train_aptos$lon)),
    ylim = c(min(train_aptos$lat), max(train_aptos$lat)),
    expand = FALSE
  ) +
  labs(title = "Ubicación de apartamentos - Muestra de entrenamiento",
       x = "Longitud", y = "Latitud") +
  theme_minimal()
```



Variables en descripción; Tokenizar

```{r}
library(tidytext)
library(dplyr)

# Tokenización de la columna 'description'
train_localidades_tokens <- train_localidades %>%
  unnest_tokens(word, description)
#Quitar stop words
data("stop_words")
train_localidades_tokens_clean <- train_localidades_tokens %>%
  anti_join(stop_words)

```

Palabras más comunes

```{r}
train_localidades_tokens_clean %>%
  count(word, sort = TRUE)
```

```{r}
#train_localidades_tokens_clean %>%
  #count(property_id, word) %>%
  #spread(key = word, value = n, fill = 0)
```

Creación variables externas desde la descripción

```{r}
library(dplyr)

#Vector palabras agrupadas
parking <- c("parqueadero", "garaje", "parqueaderos", "garajes")
terrace <- c("terraza", "balcon")
gym <- "gimnasio"
laundry <- "lavanderia"
elevator <- "ascensor"

#Variable dummy para cada caracteristica
dummies_df <- train_localidades_tokens_clean %>%
  mutate(binaria_parking = word %in% parking,
         binaria_terrace = word %in% terrace,
         binaria_gym = word %in% gym,
         binaria_laundry = word %in% laundry,
         binaria_elevator = word %in% elevator) %>%
  group_by(property_id) %>%
  summarise(across(starts_with("binaria_"), ~as.integer(any(.))))

# Unir a df 
train_localidades <- left_join(train_localidades, dummies_df, by = "property_id")

```

1)  Identificar distancia a Mall

```{r}
# Obtener los límites de Bogotá
limites_bogota <- getbb("Bogotá, Colombia")

mall_bogota <- opq(bbox = limites_bogota) %>%
  add_osm_feature(key = "shop", 
                  value = "mall")

# Descargar datos de Mall
mall_data_bogota <- osmdata_sf(mall_bogota)

# Extraer los puntos de los parques en Bogotá como un objeto sf
mall_sf_bogota <- mall_data_bogota$osm_points %>%
  dplyr::select(osm_id, name) %>%
  st_transform(crs = 4326)  # Asegurarse de que el CRS coincida con el de los apartamentos

# Revisar los datos de los parques
head(mall_sf_bogota)

#¡¡¡IMPORTANTE!!!!
#(MAGNA-SIRGAS / Colombia Bogota)

# Localidades - Transformar a un sistema de coordenadas proyectado para Colombia 
train_localidades_metros <- st_transform(train_localidades_sf,
                               crs = 3116)
print(train_localidades_metros)

# Parques - Transformar a un sistema de coordenadas proyectado para Colombia 
train_mall_metros <- st_transform(mall_sf_bogota,
                               crs = 3116)
print(train_mall_metros)

# Calcular las distancias entre los apartamentos y los parques
dist_matrix_mall <- st_distance(train_localidades_metros, train_mall_metros)

# Calcular la distancia mínima a un parque para cada apartamento
dist_min_mall <- apply(dist_matrix_mall, 1, min)

# Agregar la distancia mínima a la base de datos train_localidades
train_localidades_metros$distancia_mall <- as.numeric(dist_min_mall)

# Agregar la distancia mínima a la base de datos
train_localidades <- train_localidades %>%
  mutate(distancia_mall = dist_min_mall)

# Revisar resultados
head(train_localidades)


```

Visualizar gráfico

```{r}

# Crear histograma
p2 <- ggplot(train_localidades, aes(x = distancia_mall)) +
  geom_histogram(bins = 50, fill = "darkgreen", alpha = 0.5) +
  labs(x = "Distancia mínima hasta el Mall(m)", y = "Cantidad",
       title = "Distribución de distancia a Mall - resto de Bogotá") +
  theme_bw()

ggsave("../04_Views/GRAFICO8B-DISTANCIAMALL_RESTO.png", plot = p2, dpi = 150, width = 8, height = 6)

```

2)  Identificar distancia a CAI

```{r}

# Obtener los CAIs (estaciones de policía) en Bogotá desde OSM
cai_bogota <- opq(bbox = limites_bogota) %>%
  add_osm_feature(key = "amenity", value = "police")

# Descargar datos de Cai
cai_data_bogota <- osmdata_sf(cai_bogota)
# Extraer los puntos de los parques en Bogotá como un objeto sf
cai_sf_bogota <- cai_data_bogota$osm_points %>%
  dplyr::select(osm_id, name) %>%
  st_transform(crs = 4326)  # Asegurarse de que el CRS coincida con el de los apartamentos

# Revisar los datos de los CAI's

head(cai_sf_bogota)
#¡¡¡IMPORTANTE!!!!
#(MAGNA-SIRGAS / Colombia Bogota)
# Localidades - Transformar a un sistema de coordenadas proyectado para Colombia 
train_localidades_metros <- st_transform(train_localidades_sf,
                               crs = 3116)
print(train_localidades_metros)
# Parques - Transformar a un sistema de coordenadas proyectado para Colombia 
train_cai_metros <- st_transform(cai_sf_bogota,
                               crs = 3116)
print(train_cai_metros)
# Calcular las distancias entre los apartamentos y los parques
dist_matrix_cai <- st_distance(train_localidades_metros, train_cai_metros)
# Calcular la distancia mínima a un parque para cada apartamento
dist_min_cai <- apply(dist_matrix_cai, 1, min)
# Agregar la distancia mínima a la base de datos train_localidades
train_localidades_metros$distancia_cai <- as.numeric(dist_min_cai)
# Agregar la distancia mínima a la base de datos
train_localidades <- train_localidades %>%
  mutate(distancia_cai = dist_min_cai)
# Revisar resultados
head(train_localidades)

```

Gráfico de distancia a los CAIS

```{r}
# Visualizar las distancias a los CAI
ggplot(train_localidades, aes(x = distancia_cai)) +
  geom_histogram(bins = 50, fill = "darkred", alpha = 0.5) +
  labs(x = "Distancia mínima a un CAI (m)", y = "Cantidad",
       title = "Distribución de la distancia a los CAI en el resto de Bogotá") +
  theme_bw()

# Guardar el gráfico
ggsave("../04_Views/GRAFICO8C-DISTANCIACAI_RESTO.png", dpi = 150, width = 8, height = 6)
```

3)  Identificar distancia hasta los paraderos de buses

```{r}
# 1. Obtener las paradas de bus en Bogotá desde OSM
bus_stop_bogota <- opq(bbox = limites_bogota) %>%
  add_osm_feature(key = "highway", value = "bus_stop")

# 2. Descargar los datos
bus_stop_data <- osmdata_sf(bus_stop_bogota)

# 3. Extraer los puntos como objeto sf
bus_stop_sf <- bus_stop_data$osm_points %>%
  dplyr::select(osm_id, name) %>%
  st_transform(crs = 4326)

# 4. Transformar las capas al sistema de coordenadas proyectado (MAGNA-SIRGAS)
train_localidades_metros <- st_transform(train_localidades_sf, crs = 3116)
bus_stop_metros <- st_transform(bus_stop_sf, crs = 3116)

# 5. Calcular matriz de distancias entre localidades y paradas de bus
dist_matrix_bus <- st_distance(train_localidades_metros, bus_stop_metros)

# 6. Calcular la distancia mínima a una parada de bus
dist_min_bus <- apply(dist_matrix_bus, 1, min)

# 7. Agregar a la base de datos
train_localidades_metros$distancia_bus <- as.numeric(dist_min_bus)

# 8. Agregar a train_localidades (si aún trabajas con versión en CRS 4326)
train_localidades <- train_localidades %>%
  mutate(distancia_bus = dist_min_bus)

# 9. Revisar resultados
head(train_localidades)

```

Visualizar gráfico

```{r}
library(scales)

ggplot(train_localidades, aes(x = distancia_bus)) +
  geom_histogram(bins = 50, fill = "steelblue", alpha = 0.6) +
  labs(x = "Distancia mínima a una parada de bus (m)", y = "Cantidad",
       title = "Distribución de la distancia a paradas de bus en el resto de Bogotá") +
  scale_x_continuous(labels = comma) +
  theme_bw()

# Guardar gráfico
ggsave("../04_Views/GRAFICO8D-DISTANCIABUS_RESTO.png", dpi = 150, width = 8, height = 6)

```

4)  Identificar distancia a avenidas principales

```{r}

# 1. Obtener avenidas principales en Bogotá desde OSM
avenidas_bogota <- opq(bbox = limites_bogota) %>%
  add_osm_feature(key = "highway", 
                  value = "primary")

# 2. Descargar los datos
avenidas_data <- osmdata_sf(avenidas_bogota)

# 3. Extraer las líneas (vías suelen estar como `osm_lines`)
avenidas_sf <- avenidas_data$osm_lines %>%
  dplyr::select(osm_id, name, highway) %>%
  st_transform(crs = 4326)

# 4. Transformar al sistema proyectado para cálculo de distancias
avenidas_metros <- st_transform(avenidas_sf, crs = 3116)
train_localidades_metros <- st_transform(train_localidades_sf, crs = 3116)

# 5. Calcular la distancia de cada punto a la vía más cercana
dist_matrix_avenidas <- st_distance(train_localidades_metros, avenidas_metros)

# 6. Distancia mínima a una avenida para cada punto
dist_min_avenida <- apply(dist_matrix_avenidas, 1, min)

# 7. Agregar a la base de datos
train_localidades_metros$distancia_avenida <- as.numeric(dist_min_avenida)

train_localidades <- train_localidades %>%
  mutate(distancia_avenida = dist_min_avenida)

# 8. Revisar resultados
head(train_localidades)
```

```{r}
#1. Verificar y asegurar que sea un objeto espacial con CRS 3116
if(!inherits(train_localidades, "sf")) {
  train_localidades_sf <- st_as_sf(
    train_localidades,
    coords = c("lon", "lat"),
    crs = 3116
  )
} else {
  train_localidades_sf <- train_localidades
}

#2. Verificar el CRS este correcto
print(st_crs(train_localidades_sf))
```

Visualizar gráfico

```{r}
ggplot(train_localidades, aes(x = distancia_avenida)) +
  geom_histogram(bins = 50, fill = "darkblue", alpha = 0.6) +
  labs(x = "Distancia mínima a una avenida principal (m)", y = "Cantidad",
       title = "Distribución de la distancia a avenidas principales") +
  theme_minimal()

# Guardar gráfico 
ggsave("../04_Views/GRAFICO8D-DISTANCIAAVENIDA_RESTO.png", dpi = 150, width = 8, height = 6)
```

## 1.3 Inspección y tratamiento de BD test

```{r}
# Nombre de las columnas de la base train_hogares
colnames(test_aptos)

# Seleccion de la columnas id de hogares
test_aptos %>%
  select(property_id) %>%
  head()

# Resumen de apartamentos de la base train_hogares
skim(test_aptos)

# Imputación de missing values con la media de train_aptos

## 1. Variable surface_total con mediana de train
test_aptos <- test_aptos  %>%
  mutate(surface_total = ifelse(is.na(surface_total) == TRUE, mediana_surface_total, surface_total))

## 2. Variable surface_covered con mediana de train
test_aptos <- test_aptos  %>%
  mutate(surface_covered = ifelse(is.na(surface_covered) == TRUE, mediana_surface_covered, surface_covered))

## 3.  Variable rooms con la moda de train
test_aptos <- test_aptos  %>%
  mutate(rooms = ifelse(is.na(rooms) == TRUE, moda_rooms, rooms))

## 4.Variable bathrooms con moda de train
test_aptos <- test_aptos  %>%
  mutate(bathrooms = ifelse(is.na(bathrooms) == TRUE, moda_bathrooms, bathrooms))

# Validación de correcta imputación de missing values en test
skim(test_aptos)
```

Seleccionar las viviendas de la localidad de Chapinero para Test

```{r}
# Asegurarte de que lon y lat sean numéricos
test_aptos <- test_aptos %>%
  mutate(lon = as.numeric(lon), lat = as.numeric(lat))

# Obtener límites de Chapinero
limite_test <- getbb("Bogotá, Colombia") #Cambio

# Filtrar SOLO los que están dentro de Chapinero
test_aptos <- test_aptos %>% #Cambio
  filter(
    between(lon, limite_test[1, "min"], limite_test[1, "max"]) & #Cambio
    between(lat, limite_test[2, "min"], limite_test[2, "max"]) #Cambio
  )

# Crear objeto sf (opcional, si vas a usarlo como spatial)
test_aptos_sf <- st_as_sf(test_aptos, #Cambio
                              coords = c("lon", "lat"), 
                              crs = 4326)

# Visualizar en el mapa
leaflet() %>%
  addTiles() %>%
  addCircles(lng = test_aptos$lon, #Cambio
             lat = test_aptos$lat) #Cambio


```

1)  Distancia a centros comerciales

```{r}

# Transformar test_chapinero a objeto sf si aún no está
test_aptos_sf <- st_as_sf(test_aptos,
                              coords = c("lon", "lat"),
                              crs = 4326)

# Transformar a sistema proyectado colombiano (MAGNA-SIRGAS)
test_aptos_metros <- st_transform(test_aptos_sf, crs = 3116)

# Transformar también los malls si no lo has hecho aún (puedes omitir si ya existe)
mall_sf_bogota_metros <- st_transform(mall_sf_bogota, crs = 3116)

# Calcular matriz de distancias
dist_matrix_mall_test <- st_distance(test_aptos_metros, mall_sf_bogota_metros)

# Calcular la distancia mínima a un mall para cada apartamento en test
dist_min_mall_test <- apply(dist_matrix_mall_test, 1, min)

# Agregar la distancia mínima a la base de datos test_chapinero
test_aptos_metros$distancia_mall <- as.numeric(dist_min_mall_test)

# Agregar también la distancia como columna simple (opcional, para df sin geometría)
test_aptos <- test_aptos %>%
  mutate(distancia_mall = dist_min_mall_test)

# Revisar resultados
head(test_aptos)


```

2)  Distancia a CAI

```{r}
# Transformar test_chapinero a objeto sf si aún no está
test_aptos_sf <- st_as_sf(test_aptos,
                              coords = c("lon", "lat"),
                              crs = 4326)

# Transformar a sistema de coordenadas proyectado (MAGNA-SIRGAS)
test_aptos_metros <- st_transform(test_aptos_sf, crs = 3116)

# Transformar los CAIs a CRS 3116 si no lo hiciste antes
cai_sf_bogota_metros <- st_transform(cai_sf_bogota, crs = 3116)

# Calcular la matriz de distancias
dist_matrix_cai_test <- st_distance(test_aptos_metros, cai_sf_bogota_metros)

# Calcular la distancia mínima al CAI más cercano para cada apartamento
dist_min_cai_test <- apply(dist_matrix_cai_test, 1, min)

# Agregar la distancia mínima como nueva columna
test_aptos_metros$distancia_cai <- as.numeric(dist_min_cai_test)

# Agregar la distancia también al data frame base (sin geometría)
test_aptos <- test_aptos %>%
  mutate(distancia_cai = dist_min_cai_test)

# Verificar
head(test_aptos)


```

3)  Identificar distancia hasta los paraderos de buses

```{r}
# Asegúrate de tener test_chapinero_sf como objeto sf
test_aptos_sf <- st_as_sf(test_aptos,
                              coords = c("lon", "lat"),
                              crs = 4326)

# Transformar a sistema de coordenadas proyectado (MAGNA-SIRGAS / Bogotá)
test_aptos_metros <- st_transform(test_aptos_sf, crs = 3116)

# Asegúrate de tener las paradas de bus transformadas a CRS 3116
bus_stop_metros <- st_transform(bus_stop_sf, crs = 3116)

# Calcular la matriz de distancias entre apartamentos y paradas de bus
dist_matrix_bus_test <- st_distance(test_aptos_metros, bus_stop_metros)

# Calcular la distancia mínima a una parada de bus
dist_min_bus_test <- apply(dist_matrix_bus_test, 1, min)

# Agregar la distancia mínima a test_chapinero_metros
test_aptos_metros$distancia_bus <- as.numeric(dist_min_bus_test)

# Agregar la distancia a la versión sin geometría
test_aptos <- test_aptos %>%
  mutate(distancia_bus = dist_min_bus_test)

# Verificar
head(test_aptos)

```

4)  Identificar distancia a avenidas principales

```{r}
# Asegúrate de que test_chapinero_sf tenga geometría (EPSG:4326)
test_aptos_sf <- st_as_sf(test_aptos,
                              coords = c("lon", "lat"),
                              crs = 4326)

# 1. Transformar los datos al sistema de coordenadas proyectado (MAGNA-SIRGAS)
test_aptos_metros <- st_transform(test_aptos_sf, crs = 3116)

# 2. Asegúrate de tener avenidas_metros definido previamente
# (Ya creado en tu código anterior como st_transform(avenidas_sf, crs = 3116))

# 3. Calcular la matriz de distancias entre apartamentos y avenidas principales
dist_matrix_avenidas_test <- st_distance(test_aptos_metros, avenidas_metros)

# 4. Calcular la distancia mínima a una avenida
dist_min_avenida_test <- apply(dist_matrix_avenidas_test, 1, min)

# 5. Agregar la distancia mínima a test_chapinero_metros
test_aptos_metros$distancia_avenida <- as.numeric(dist_min_avenida_test)

# 6. Agregar también a la base de datos test_chapinero sin geometría
test_aptos <- test_aptos %>%
  mutate(distancia_avenida = dist_min_avenida_test)

# 7. Verificar resultados
head(test_aptos)

```

Variables en descripción; Tokenizar

```{r}
library(tidytext)
library(dplyr)

# Tokenización de la columna 'description'
test_aptos_tokens <- test_aptos %>%
  unnest_tokens(word, description)

# Quitar stop words
data("stop_words")
test_aptos_tokens_clean <- test_aptos_tokens %>%
  anti_join(stop_words)

```

Palabras más comunes

```{r}
test_aptos_tokens_clean %>%
  count(word, sort = TRUE)
```

Creación variables externas desde la descripción

```{r}

library(dplyr)

#Vector palabras agrupadas
parking <- c("parqueadero", "garaje", "parqueaderos", "garajes")
terrace <- c("terraza", "balcon")
gym <- "gimnasio"
laundry <- "lavanderia"
elevator <- "ascensor"

#Variable dummy para cada caracteristica
dummies_df_test <- test_aptos_tokens_clean %>%
  mutate(binaria_parking = word %in% parking,
         binaria_terrace = word %in% terrace,
         binaria_gym = word %in% gym,
         binaria_laundry = word %in% laundry,
         binaria_elevator = word %in% elevator) %>%
  group_by(property_id) %>%
  summarise(across(starts_with("binaria_"), ~as.integer(any(.))))

# Unir a df 
test_aptos <- left_join(test_aptos, dummies_df_test, by = "property_id")
```
-###--------#####
## 1.4 Convertir a factores base entrenamiento

```{r}
# Variables categóricas a convertir en factor
cols_factor <- c(
  'year',
  'property_type',
  'binaria_parking',
  'binaria_terrace',
  'binaria_gym',
  'binaria_laundry',
  'binaria_elevator'
)##OK

# Filtrar y preparar datos
train_factors <- train_localidades %>%
  select(
    property_id,
    price,
    year,
    property_type,
    rooms,
    bedrooms,
    bathrooms,
    surface_total,
    surface_covered,
    distancia_cai,
    distancia_mall,
    distancia_bus,
    distancia_avenida,
    binaria_parking,
    binaria_terrace,
    binaria_gym,
    binaria_laundry,
    binaria_elevator,
    lat,
    lon
  ) %>%
  mutate(across(all_of(cols_factor), as.factor)) %>%
  as.data.frame()

# Quitar NAs del dataframe preparado
train_factors <- na.omit(train_factors) ## Esta es la base que se usará para el entrenamiento

# Variables predictoras
variables_predictoras <- c(
  'year',
  'rooms',
  'bedrooms',
  'bathrooms',
  'surface_total',
  'surface_covered',
  'property_type',
  'distancia_cai',
  'distancia_mall',
  'distancia_bus',
  'distancia_avenida',
  'binaria_parking',
  'binaria_terrace',
  'binaria_gym',
  'binaria_laundry',
  'binaria_elevator'
)

# Definir fórmula del modelo
variables <- paste(variables_predictoras, collapse = " + ")
formula_modelo <- as.formula(paste("price ~", variables))

```

Convertir a factores base test

```{r}
# Las columnas estén en el mismo formato que en entrenamiento
cols_factor <- c(
  'year',
  'property_type',
  'binaria_parking',
  'binaria_terrace',
  'binaria_gym',
  'binaria_laundry',
  'binaria_elevator'
)

# Conservar test con `price` para evaluación posterior
test <- test_aptos %>%
  select(
    property_id,
    year,
    property_type,
    rooms,
    bedrooms,
    bathrooms,
    surface_total,
    surface_covered,
    distancia_cai,
    distancia_mall,
    distancia_bus,
    distancia_avenida,
    binaria_parking,
    binaria_terrace,
    binaria_gym,
    binaria_laundry,
    binaria_elevator,
    lat,
    lon
  ) %>%
  mutate(across(all_of(cols_factor), as.factor)) %>%
  na.omit() %>%
  as.data.frame()

```

# 2. Entrenamiento de modelos

## 2.1.A Elastic net CV Espacial

### 2.1.1.A Entrenamiento

```{r}
#1. Eliminar variables innecesarias
train_1_EN <- train_factors %>%
  select(-property_id, -lat, -lon) #Quitar property_id y lat, lon

#2. Crear la receta del entranamiento
# Cargar explícitamente el paquete recipes
receta_1_EN <- recipe(price ~ ., data = train_1_EN) %>%
  step_normalize(distancia_cai, distancia_mall, distancia_bus, distancia_avenida) %>% #normalizar solo predictores numéricos (distancias)
  step_dummy(all_nominal_predictors()) %>% ## Convertir variables categóricas a dummies
  step_zv(all_predictors()) #Remover predictores con varianza cero

#3. Definir el modelo
modelo_1_EN <- linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet") 

#4. Especificar los paramatros alpha y lambda EN
#Estamos creando la cuadricula de búsqueda para los hiperparámetros del modelo Elastric Net
#Penalty: lambda que controla la regularización
#Mixture: alpha es el balance entre la regularizacion L1 (Lasso) y L2 (Ridge)

grid_elastic_1 <- grid_regular(
  penalty(range = c(-4, -1), trans = log10_trans()),  # Explorando valores desde 0.0001 hasta 1 en escala logaritmica
  mixture(range = c(0, 1)),                          # Explorar de 0 a 1
  levels = c(15, 8)                                  # 15 niveles de penalty, 8 de mixture
)

#5. Crear un workflow (flujo de trabajo)

workflow_1_EN <- workflow() %>%
  add_recipe(receta_1_EN) %>%
  add_model(modelo_1_EN)

#6. Unir train_factors con train_localidades_sf por medio de property_id

train_espacial <- train_localidades_sf %>%
  select(property_id, geometry) %>%
  inner_join(train_factors, by = "property_id")

#7. Estimacion de validacion cruzada espacial por bloques
#Este metodo de VC por bloques permite:
  #1. Mantener la independencia espacial entre los bloques
  #2. Evitar el sobreajuste o estimaciones demasiado optimistas del rendimiento del modelo
library(spatialsample)
set.seed(123)
block_folds_EN <- spatial_block_cv(train_espacial, v = 5)

#8. Verificación visual de los bloques de la valiación cruzada
autoplot(block_folds_EN)

#9. Incluir la libreria purrr
#Vamos a ver el proceso de la validacion cruzada 
library(purrr)
walk(block_folds_EN$splits, function(x) print(autoplot(x)))

#10. Entrenamiento del modelo con validacion cruzada espacial
resultados_1_EN <- workflow_1_EN %>%
  tune_grid(
    resamples = block_folds_EN,
    grid = grid_elastic_1,
    metrics = metric_set(mae),
    control = control_grid(verbose = TRUE)
  )
#11. Ver los resultados del modelo 1
autoplot(resultados_1_EN)

#12. Resultado del modelo 1
collect_metrics(resultados_1_EN)

#13. Mostrar los mejores resultados
show_best(resultados_1_EN, metric = "mae", n = 5)

#14. Seleccionar los mejores hiperparámetros
best_params_1 <- select_best(resultados_1_EN, metric = "mae")
print(best_params_1)

#15. Finalizar el modelo con los mejores parámetros
#Se realiza la implementación de los mejores parámetros en el workflow
#¡¡¡IMPORTANTE!!!
#La función finalize_workflow() sustituye los placeholders tune() por los valores específicos de best_params.
#Ademas, reemplaza penalty = tune() y mixture = tune() por los valores numéricos que resultaron óptimos según el criterio de MAE.
#Por lo tanto, final_workflow estan todos los parámetros especificados con valores estimados, listo para ser entrenado en el conjunto completo de datos.
final_workflow_1 <- workflow_1_EN %>%
  finalize_workflow(best_params_1)

#16. Entrenar el modelo final con todos los datos
final_model_1 <- fit(final_workflow_1, data = train_1_EN)

#17. Analizar los coeficientes del modelo
# Ver todos los coeficientes (ordenados por magnitud)
library(broom)
coeficientes_1 <- tidy(final_model_1) %>%
  filter(term != "(Intercept)") %>%
  arrange(desc(abs(estimate)))

#18. Ver los coeficientes más importantes
head(coeficientes_1, 10)

#19. Ver la importancia de las variables
library(vip)
vip(extract_fit_parsnip(final_model_1), num_features = 20)

#20. Calcular el coeficiente de variación de los precios reales
cv_precios_reales_1 <- sd(train_1_EN$price) / mean(train_1_EN$price) * 100
print(paste("CV de los precios reales (%):", round(cv_precios_reales_1, 2)))

```

### 2.1.2.A Predicción

```{r}
#1. Tomar la base de test y prepararla para la predicción
na_conteo_test <- colSums(is.na(test))
print(na_conteo_test)

test_1_EN <- test %>%
  na.omit()

str(test_1_EN)
print(dim(test_1_EN))

#2. Realizar predicciones en el conjunto de prueba
prediccion_1_EN <- predict(final_model_1, new_data = test_1_EN)

resultado_EN_1 <- test_1_EN %>%
  select(property_id) %>%
  bind_cols(prediccion_1_EN)%>%
  rename(price = .pred)

head(resultado_EN_1)

#3. Ver la distribución de las predicciones
ggplot(prediccion_1_EN, aes(x = .pred/1000000)) +  # Convertir a millones
  geom_histogram(bins = 30, fill = "skyblue", color = "darkblue") +
  labs(title = "Distribución de las predicciones de precio",
       x = "Precio predicho (Millones de Pesos)",
       y = "Frecuencia") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),  # Centrar título
    axis.text.x = element_text(size = 9)
  ) +
  scale_x_continuous(
    labels = scales::comma_format(accuracy = 0.1),  # Formato de números con comas y 1 decimal
    breaks = scales::pretty_breaks(n = 8)           # Número adecuado de marcas
  )
#4. Calcular estadísticas básicas de las predicciones
summary(prediccion_1_EN$.pred)

#5. Calcular el coeficiente de variación de las predicciones
cv_predicciones_1_EN <- sd(prediccion_1_EN$.pred) / mean(prediccion_1_EN$.pred) * 100
print(paste("Coeficiente de variación de las predicciones (%):", round(cv_predicciones_1_EN, 2)))

#6. Unir predicciones con otras variables

analisis_segmentos_1_EN <- test_1_EN %>%
  select(property_type) %>%
  bind_cols(prediccion_1_EN) %>%
  group_by(property_type) %>%
  summarize(
    media = mean(.pred),
    cv = (sd(.pred) / mean(.pred)) * 100
  )

print(analisis_segmentos_1_EN)

#7. Identificar posibles valores atípicos
umbral_superior_1 <- quantile(prediccion_1_EN$.pred, 0.975)
umbral_inferior_1 <- quantile(prediccion_1_EN$.pred, 0.025)

outliers_1 <- prediccion_1_EN %>%
  filter(.pred > umbral_superior_1 | .pred < umbral_inferior_1)

#8. Visualizar los outliers
print(paste("Número de outliers en predicciones:", nrow(outliers_1)))

```

### 2.1.3.A Template Kaggle

```{r}
# Descargar archivo
write.csv(resultado_EN_1,
          "../03_Stores/18-05_G4_EN1.csv", row.names = FALSE, 
          quote = FALSE)
```
## 2.1.B Elastic net CV Convencional

### 2.1.1.B Entrenamiento

```{r}
#1. Eliminar variables innecesarias
train_2_EN <- train_factors %>%
  select(-property_id, -lat, -lon) #Quitar property_id y lat, lon

#2. Crear la receta del entranamiento
# Cargar explícitamente el paquete recipes
receta_2_EN <- recipe(price ~ ., data = train_2_EN) %>%
  step_normalize(distancia_cai, distancia_mall, distancia_bus, distancia_avenida) %>% #normalizar solo predictores numéricos (distancias)
  step_dummy(all_nominal_predictors()) %>% ## Convertir variables categóricas a dummies
  step_zv(all_predictors()) #Remover predictores con varianza cero

#3. Definir el modelo
modelo_2_EN <- linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet") 

#4. Especificar los paramatros alpha y lambda EN
#Estamos creando la cuadricula de búsqueda para los hiperparámetros del modelo Elastric Net
#Penalty: lambda que controla la regularización
#Mixture: alpha es el balance entre la regularizacion L1 (Lasso) y L2 (Ridge)

grid_elastic_2 <- grid_regular(
  penalty(range = c(-4, -1), trans = log10_trans()),  # Explorando valores desde 0.001 hasta 1 en escala logaritmica
  mixture(range = c(0, 1)),                          # Explorar de 0 a 1
  levels = c(15, 8)                                  # 10 niveles de penalty, 5 de mixture
)

#5. Crear un workflow (flujo de trabajo)

workflow_2_EN <- workflow() %>%
  add_recipe(receta_2_EN) %>%
  add_model(modelo_2_EN)

#6. Unir train_factors con train_localidades_sf por medio de property_id

train_espacial <- train_localidades_sf %>%
  select(property_id, geometry) %>%
  inner_join(train_factors, by = "property_id")

#7. Estimacion de validacion cruzada convencional
#Este metodo de VC permite:
  #1. División aleatoria de los datos en k partes
  #2. Evaluación estándar de rendimiento del modelo

library(rsample)
set.seed(123)
cv_folds_EN <- vfold_cv(train_2_EN, 
                        v = 5,
                        strata = price) # Estratificar por precio

#8. Verificación visual de los folds de la valiación cruzada
print(cv_folds_EN)

#9. Ver información de los folds
#Vamos a ver el proceso de la validacion cruzada 
library(purrr)
map_df(cv_folds_EN$splits,~c(
  train = nrow(training(.x)),
  test= nrow(assessment(.x))
  ))

#10. Entrenamiento del modelo con validacion cruzada espacial
resultados_2_EN <- workflow_2_EN %>%
  tune_grid(
    resamples = cv_folds_EN,
    grid = grid_elastic_2,
    metrics = metric_set(mae),
    control = control_grid(verbose = TRUE)
  )
#11. Ver los resultados del modelo 1
autoplot(resultados_2_EN)

#12. Resultado del modelo 1
collect_metrics(resultados_2_EN)

#13. Mostrar los mejores resultados
show_best(resultados_2_EN, metric = "mae", n = 5)

#14. Seleccionar los mejores hiperparámetros
best_params_2 <- select_best(resultados_2_EN, metric = "mae")
print(best_params_2)

#15. Finalizar el modelo con los mejores parámetros
#Se realiza la implementación de los mejores parámetros en el workflow
#¡¡¡IMPORTANTE!!!
#La función finalize_workflow() sustituye los placeholders tune() por los valores específicos de best_params.
#Ademas, reemplaza penalty = tune() y mixture = tune() por los valores numéricos que resultaron óptimos según el criterio de MAE.
#Por lo tanto, final_workflow estan todos los parámetros especificados con valores estimados, listo para ser entrenado en el conjunto completo de datos.
final_workflow_2 <- workflow_2_EN %>%
  finalize_workflow(best_params_2)

#16. Entrenar el modelo final con todos los datos
final_model_2 <- fit(final_workflow_2, data = train_2_EN)

#17. Analizar los coeficientes del modelo
# Ver todos los coeficientes (ordenados por magnitud)
library(broom)
coeficientes_2 <- tidy(final_model_2) %>%
  filter(term != "(Intercept)") %>%
  arrange(desc(abs(estimate)))

#18. Ver los coeficientes más importantes
head(coeficientes_2, 10)

#19. Ver la importancia de las variables
library(vip)
vip(extract_fit_parsnip(final_model_2), num_features = 20)

#20. Calcular el coeficiente de variación de los precios reales
cv_precios_reales_2 <- sd(train_2_EN$price) / mean(train_2_EN$price) * 100
print(paste("CV de los precios reales (%):", round(cv_precios_reales_2, 2)))

```
### 2.1.2.B Predicción

```{r}
#1. Tomar la base de test y prepararla para la predicción
na_conteo_test <- colSums(is.na(test))
print(na_conteo_test)

test_2_EN <- test %>%
  na.omit()

str(test_2_EN)
print(dim(test_2_EN))

#2. Realizar predicciones en el conjunto de prueba
prediccion_2_EN <- predict(final_model_2, new_data = test_2_EN)

resultado_EN_2 <- test_2_EN %>%
  select(property_id) %>%
  bind_cols(prediccion_2_EN)%>%
  rename(price = .pred)

head(resultado_EN_2)

#3. Ver la distribución de las predicciones
ggplot(prediccion_2_EN, aes(x = .pred/1000000)) +  # Convertir a millones
  geom_histogram(bins = 30, fill = "skyblue", color = "darkblue") +
  labs(title = "Distribución de las predicciones de precio",
       x = "Precio predicho (Millones de Pesos)",
       y = "Frecuencia") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),  # Centrar título
    axis.text.x = element_text(size = 9)
  ) +
  scale_x_continuous(
    labels = scales::comma_format(accuracy = 0.1),  # Formato de números con comas y 1 decimal
    breaks = scales::pretty_breaks(n = 8)           # Número adecuado de marcas
  )
#4. Calcular estadísticas básicas de las predicciones
summary(prediccion_2_EN$.pred)

#5. Calcular el coeficiente de variación de las predicciones
cv_predicciones_2_EN <- sd(prediccion_2_EN$.pred) / mean(prediccion_2_EN$.pred) * 100
print(paste("Coeficiente de variación de las predicciones (%):", round(cv_predicciones_2_EN, 2)))

#6. Unir predicciones con otras variables

analisis_segmentos_2_EN <- test_2_EN %>%
  select(property_type) %>%
  bind_cols(prediccion_2_EN) %>%
  group_by(property_type) %>%
  summarize(
    media = mean(.pred),
    cv = (sd(.pred) / mean(.pred)) * 100
  )

print(analisis_segmentos_2_EN)

#7. Identificar posibles valores atípicos
umbral_superior_2 <- quantile(prediccion_2_EN$.pred, 0.975)
umbral_inferior_2 <- quantile(prediccion_2_EN$.pred, 0.025)

outliers_2 <- prediccion_2_EN %>%
  filter(.pred > umbral_superior_2 | .pred < umbral_inferior_2)

#8. Visualizar los outliers
print(paste("Número de outliers en predicciones:", nrow(outliers_2)))

```
### 2.1.3.B Template Kaggle

```{r}
# Descargar archivo

write.csv(resultado_EN_2,
          "C:/../03_Stores/25-05_G4_EN2.csv", row.names = FALSE, 
          quote = FALSE)
```

## 2.1.C Comparación entre los modelos train_1_EN y train_2_EN
```{r}
# 1. Comparar las métricas MAE de validación cruzada
mae_espacial <- collect_metrics(resultados_1_EN) %>% 
  filter(.metric == "mae") %>% 
  summarise(
    mae_promedio = mean(mean),
    mae_se = mean(std_err),
    mejor_mae = min(mean)
  ) %>%
  mutate(modelo = "CV Espacial")

mae_convencional <- collect_metrics(resultados_2_EN) %>% 
  filter(.metric == "mae") %>% 
  summarise(
    mae_promedio = mean(mean),
    mae_se = mean(std_err),
    mejor_mae = min(mean)
  ) %>%
  mutate(modelo = "CV Convencional")

# Combinar resultados
comparacion_mae <- bind_rows(mae_espacial, mae_convencional)
print("Comparación de MAE entre modelos:")
print(comparacion_mae)

# 2. Comparar los mejores hiperparámetros
print("Mejores hiperparámetros - CV Espacial:")
print(best_params_1)
print("Mejores hiperparámetros - CV Convencional:")
print(best_params_2)

# 3. Crear tabla de comparación de coeficientes de variación
cv_precios_reales_1 <- sd(train_1_EN$price) / mean(train_1_EN$price) * 100
cv_precios_reales_2 <- sd(train_2_EN$price) / mean(train_2_EN$price) * 100

comparacion_cv <- data.frame(
  Modelo = c("CV Espacial", "CV Convencional"),
  CV_Precios_Reales = c(round(cv_precios_reales_1, 2), round(cv_precios_reales_2, 2)),
  Mejor_MAE = c(round(mae_espacial$mejor_mae, 0), round(mae_convencional$mejor_mae, 0)),
  MAE_Promedio = c(round(mae_espacial$mae_promedio, 0), round(mae_convencional$mae_promedio, 0))
)

print("Tabla de comparación:")
print(comparacion_cv)

# 4. Visualización comparativa de MAE
library(ggplot2)

# Gráfico de barras comparativo
ggplot(comparacion_mae, aes(x = modelo, y = mejor_mae, fill = modelo)) +
  geom_col(width = 0.6) +
  geom_text(aes(label = paste("MAE:", format(round(mejor_mae, 0), big.mark = ","))), 
            vjust = -0.5, size = 4) +
  labs(title = "Comparación del Mejor MAE entre Modelos",
       subtitle = "Validación Cruzada Espacial vs Convencional",
       x = "Tipo de Validación Cruzada",
       y = "MAE (Menor es mejor)",
       fill = "Modelo") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5),
    legend.position = "none"
  ) +
  scale_fill_manual(values = c("CV Espacial" = "skyblue", "CV Convencional" = "lightcoral")) +
  scale_y_continuous(labels = scales::comma_format())

# 5. Comparar distribución de MAE en la validación cruzada
mae_espacial_detalle <- collect_metrics(resultados_1_EN) %>% 
  filter(.metric == "mae") %>%
  mutate(modelo = "CV Espacial")

mae_convencional_detalle <- collect_metrics(resultados_2_EN) %>% 
  filter(.metric == "mae") %>%
  mutate(modelo = "CV Convencional")

mae_completo <- bind_rows(mae_espacial_detalle, mae_convencional_detalle)

# Boxplot comparativo
ggplot(mae_completo, aes(x = modelo, y = mean, fill = modelo)) +
  geom_boxplot(alpha = 0.7, width = 0.5) +
  geom_jitter(width = 0.2, alpha = 0.6, size = 2) +
  labs(title = "Distribución de MAE en Validación Cruzada",
       subtitle = "Comparación entre diferentes configuraciones de hiperparámetros",
       x = "Tipo de Validación Cruzada",
       y = "MAE",
       fill = "Modelo") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5),
    legend.position = "none"
  ) +
  scale_fill_manual(values = c("CV Espacial" = "skyblue", "CV Convencional" = "lightcoral")) +
  scale_y_continuous(labels = scales::comma_format())

# 6. Análisis estadístico de la diferencia
diferencia_mae <- mae_espacial$mejor_mae - mae_convencional$mejor_mae
diferencia_porcentual <- (diferencia_mae / mae_convencional$mejor_mae) * 100

print(paste("Diferencia en MAE:", format(round(diferencia_mae, 0), big.mark = ",")))
print(paste("Diferencia porcentual:", round(diferencia_porcentual, 2), "%"))

if(diferencia_mae > 0) {
  print("El modelo con CV Convencional tiene mejor rendimiento (menor MAE)")
} else {
  print("El modelo con CV Espacial tiene mejor rendimiento (menor MAE)")
}

# 7. Comparar los coeficientes más importantes de ambos modelos
top_coef_espacial <- head(coeficientes_1, 5) %>% 
  mutate(modelo = "CV Espacial") %>%
  select(modelo, term, estimate)

top_coef_convencional <- head(coeficientes_2, 5) %>% 
  mutate(modelo = "CV Convencional") %>%
  select(modelo, term, estimate)

print("Top 5 coeficientes - CV Espacial:")
print(top_coef_espacial)
print("Top 5 coeficientes - CV Convencional:")
print(top_coef_convencional)

```
## 2.1.D Comparación entre los modelos test_1_EN y test_2_EN

```{r}
### 2.4 Comparación de Predicciones: CV Espacial vs CV Convencional

#1. Comparación de estadísticas descriptivas de las predicciones
print("=== COMPARACIÓN DE ESTADÍSTICAS DE PREDICCIONES ===")

# Resumen estadístico comparativo
comparacion_stats <- data.frame(
  Modelo = c("CV Espacial", "CV Convencional"),
  Media = c(mean(prediccion_1_EN$.pred), mean(prediccion_2_EN$.pred)),
  Mediana = c(median(prediccion_1_EN$.pred), median(prediccion_2_EN$.pred)),
  Desv_Std = c(sd(prediccion_1_EN$.pred), sd(prediccion_2_EN$.pred)),
  Min = c(min(prediccion_1_EN$.pred), min(prediccion_2_EN$.pred)),
  Max = c(max(prediccion_1_EN$.pred), max(prediccion_2_EN$.pred)),
  CV_Predicciones = c(cv_predicciones_1_EN, cv_predicciones_2_EN),
  Num_Outliers = c(nrow(outliers_1), nrow(outliers_2))
)

# Formatear números grandes
comparacion_stats_formateada <- comparacion_stats %>%
  mutate(
    Media = format(round(Media, 0), big.mark = ","),
    Mediana = format(round(Mediana, 0), big.mark = ","),
    Desv_Std = format(round(Desv_Std, 0), big.mark = ","),
    Min = format(round(Min, 0), big.mark = ","),
    Max = format(round(Max, 0), big.mark = ","),
    CV_Predicciones = round(CV_Predicciones, 2)
  )

print(comparacion_stats_formateada)

#2. Comparación de coeficientes de variación
print("=== COMPARACIÓN DE COEFICIENTES DE VARIACIÓN ===")
print(paste("CV Espacial:", round(cv_predicciones_1_EN, 2), "%"))
print(paste("CV Convencional:", round(cv_predicciones_2_EN, 2), "%"))

diferencia_cv <- cv_predicciones_1_EN - cv_predicciones_2_EN
print(paste("Diferencia en CV:", round(diferencia_cv, 2), "puntos porcentuales"))

if(abs(diferencia_cv) < 0.5) {
  print("Ambos modelos tienen variabilidad similar")
} else if(cv_predicciones_1_EN < cv_predicciones_2_EN) {
  print("El modelo espacial tiene predicciones más consistentes")
} else {
  print("El modelo convencional tiene predicciones más consistentes")
}

#3. Visualización comparativa de distribuciones
library(ggplot2)

# Combinar datos para visualización
predicciones_combinadas <- bind_rows(
  prediccion_1_EN %>% mutate(Modelo = "CV Espacial"),
  prediccion_2_EN %>% mutate(Modelo = "CV Convencional")
)

# Histograma comparativo
ggplot(predicciones_combinadas, aes(x = .pred/1000000, fill = Modelo)) +
  geom_histogram(bins = 30, alpha = 0.7, position = "identity") +
  labs(title = "Comparación de Distribuciones de Predicciones",
       subtitle = "CV Espacial vs CV Convencional",
       x = "Precio predicho (Millones de Pesos)",
       y = "Frecuencia",
       fill = "Modelo") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5),
    legend.position = "top"
  ) +
  scale_fill_manual(values = c("CV Espacial" = "skyblue", "CV Convencional" = "lightcoral")) +
  scale_x_continuous(
    labels = scales::comma_format(accuracy = 0.1),
    breaks = scales::pretty_breaks(n = 8)
  )

# Boxplot comparativo
ggplot(predicciones_combinadas, aes(x = Modelo, y = .pred/1000000, fill = Modelo)) +
  geom_boxplot(alpha = 0.7, width = 0.6) +
  stat_summary(fun = mean, geom = "point", shape = 23, size = 3, color = "red") +
  labs(title = "Comparación de Predicciones por Modelo",
       subtitle = "El rombo rojo indica la media",
       x = "Modelo",
       y = "Precio predicho (Millones de Pesos)",
       fill = "Modelo") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5),
    legend.position = "none"
  ) +
  scale_fill_manual(values = c("CV Espacial" = "skyblue", "CV Convencional" = "lightcoral")) +
  scale_y_continuous(labels = scales::comma_format(accuracy = 0.1))

#4. Comparación por tipo de propiedad
print("=== ANÁLISIS POR TIPO DE PROPIEDAD ===")

# Corregir nombres de variables (parecía haber un error en el código original)
analisis_segmentos_1_EN <- test_1_EN %>%
  select(property_type) %>%
  bind_cols(prediccion_1_EN) %>%
  group_by(property_type) %>%
  summarize(
    media = mean(.pred),
    cv = (sd(.pred) / mean(.pred)) * 100,
    .groups = 'drop'
  ) %>%
  mutate(Modelo = "CV Espacial")

analisis_segmentos_2_EN <- test_2_EN %>%
  select(property_type) %>%
  bind_cols(prediccion_2_EN) %>%
  group_by(property_type) %>%
  summarize(
    media = mean(.pred),
    cv = (sd(.pred) / mean(.pred)) * 100,
    .groups = 'drop'
  ) %>%
  mutate(Modelo = "CV Convencional")

# Combinar análisis por segmentos
analisis_completo <- bind_rows(analisis_segmentos_1_EN, analisis_segmentos_2_EN)

print("Análisis por tipo de propiedad:")
print(analisis_completo)

# Visualización por tipo de propiedad
ggplot(analisis_completo, aes(x = property_type, y = media/1000000, fill = Modelo)) +
  geom_col(position = "dodge", alpha = 0.8) +
  geom_text(aes(label = paste(round(media/1000000, 1), "M")), 
            position = position_dodge(width = 0.9), vjust = -0.5, size = 3) +
  labs(title = "Predicciones Promedio por Tipo de Propiedad",
       x = "Tipo de Propiedad",
       y = "Precio Promedio (Millones de Pesos)",
       fill = "Modelo") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    legend.position = "top"
  ) +
  scale_fill_manual(values = c("CV Espacial" = "skyblue", "CV Convencional" = "lightcoral"))

#5. Correlación entre predicciones de ambos modelos
correlacion <- cor(prediccion_1_EN$.pred, prediccion_2_EN$.pred)
print(paste("Correlación entre predicciones:", round(correlacion, 4)))

# Gráfico de dispersión
data_correlacion <- data.frame(
  Espacial = prediccion_1_EN$.pred / 1000000,
  Convencional = prediccion_2_EN$.pred / 1000000
)

ggplot(data_correlacion, aes(x = Espacial, y = Convencional)) +
  geom_point(alpha = 0.5, color = "darkblue") +
  geom_smooth(method = "lm", color = "red", linetype = "dashed") +
  geom_abline(slope = 1, intercept = 0, color = "gray", linetype = "dotted") +
  labs(title = paste("Correlación entre Predicciones (r =", round(correlacion, 3), ")"),
       x = "CV Espacial (Millones de Pesos)",
       y = "CV Convencional (Millones de Pesos)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

#6. Resumen final
print("=== RESUMEN COMPARATIVO ===")
print(paste("Diferencia promedio en predicciones:", 
            format(round(mean(prediccion_1_EN$.pred) - mean(prediccion_2_EN$.pred), 0), big.mark = ",")))

if(cv_predicciones_1_EN < cv_predicciones_2_EN) {
  print("GANADOR EN CONSISTENCIA: CV Espacial (menor CV)")
} else {
  print("GANADOR EN CONSISTENCIA: CV Convencional (menor CV)")
}

print(paste("Correlación alta (", round(correlacion, 3), ") indica que ambos modelos hacen predicciones similares"))
```


## 2.2 CARTs

### 2.2.1 Entrenamiento tradicional 
```{r}
modelo_cart <- rpart(
  formula = formula_modelo,
  data = train_factors,
  method = "anova"  # Regresión continua
)

# Visualizar el árbol
rpart.plot(modelo_cart)

```
### 2.2.2 Predicción 
```{r}
# Predecir
test$price <- predict(modelo_cart, newdata = test)

# Seleccionar columnas en orden requerido
pred_CART1 <- test %>%
  select(property_id, price)
```

```{r}
# Predecir sobre entrenamiento 
pred_train <- predict(modelo_cart, newdata = train_factors)

# Calcular MAE
mae_train <- mean(abs(train_factors$price - pred_train))
print(paste("MAE en entrenamiento:", round(mae_train, 2)))

```
por revisar 
```{r}
# Predecir
pred_test <- predict(modelo_cart, newdata = test)

library(rpart)

# Entrenar modelo CART con train_factors y fórmula definida
modelo_cart <- rpart(formula_modelo, data = train_factors)

# Predecir en test (que sí tiene columna price)
pred_test <- predict(modelo_cart, newdata = test)

# Calcular MAE ignorando NAs si hay
mae_test <- mean(abs(test$price - pred_test), na.rm = TRUE)
print(paste("MAE en test:", round(mae_test, 2)))


```

### 2.2.2 Entrenamiento Espacial
```{r}
library(recipes)
library(parsnip)
library(workflows)
library(tune)
library(spatialsample)
library(dplyr)

# 1. Receta de preprocesamiento
receta_CART <- recipe(price ~ ., data = train_1_EN) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors())

# 2. Definir modelo CART con parámetros a tunear
modelo_CART <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()
) %>%
  set_engine("rpart") %>%
  set_mode("regression")

# 3. Grid de búsqueda para hiperparámetros
grid_CART <- grid_regular(
  cost_complexity(range = c(-4, -1), trans = log10_trans()),
  tree_depth(range = c(3, 10)),
  min_n(range = c(2, 10)),
  levels = c(4, 4, 4)
)

# 4. Workflow
workflow_CART <- workflow() %>%
  add_recipe(receta_CART) %>%
  add_model(modelo_CART)

# 5. Validación cruzada espacial con 2 folds
block_folds_CART <- spatial_block_cv(train_espacial, v = 2)

# 6. Búsqueda de hiperparámetros con validación cruzada espacial
resultados_CART <- workflow_CART %>%
  tune_grid(
    resamples = block_folds_CART,
    grid = grid_CART,
    metrics = metric_set(mae),
    control = control_grid(verbose = TRUE)
  )

# 7. Resultados y mejores parámetros
show_best(resultados_CART, metric = "mae", n = 5)

mejores_CART <- select_best(resultados_CART, metric = "mae")
print(mejores_CART)

# 8. Finalizar workflow con mejores parámetros
final_workflow_CART <- finalize_workflow(workflow_CART, mejores_CART)

# 9. Ajustar modelo final con todos los datos de entrenamiento
modelo_final_CART <- fit(final_workflow_CART, data = train_1_EN)

```

### 2.2.2 Predicción ---REVISAR----
```{r}

# Realizar predicciones con el modelo final
predicciones_CART <- predict(modelo_final_CART, new_data = test_1_EN)

# Combinar property_id con las predicciones
pred_CART2 <- test_1_EN %>%
  select(property_id) %>%
  bind_cols(predicciones_CART) %>%
  rename(price = .pred)

head(pred_CART2)

```

### 2.2.3 Template Kaggle
```{r}
# Descargar archivo
write.csv(pred_CART1,
          "C:/MECA/2025/BIG DATA Y MACHINE LEARNING- Ignasio Sarmiento/Taller3/T3_BDML/03_Stores/20-05_G4_CART1.csv", row.names = FALSE, 
          quote = FALSE)

write.csv(pred_CART2,
          "C:/MECA/2025/BIG DATA Y MACHINE LEARNING- Ignasio Sarmiento/Taller3/T3_BDML/03_Stores/21-05_G4_CART2.csv", row.names = FALSE, 
          quote = FALSE)
```

## 2.3 Random Forest

### 2.3.1 Entrenamiento tradicional
```{r}
# Control de entrenamiento con out-of-bag (OOB)
fitControl <- trainControl(method = "oob")

# Grid de hiperparámetros
tree_grid <- expand.grid(
  mtry = 5,
  splitrule = "variance",       # para regresión
  min.node.size = c(2, 3, 4)
)

# Entrenar el modelo
model_forest <- train(
  formula_modelo,
  data = train_factors,
  method = "ranger",
  trControl = fitControl,
  tuneGrid = tree_grid,
  num.trees = 500,
  importance = "permutation"
)

# Resultados del modelo
print(model_forest)

# Importancia de variables
varImp(model_forest)

```

### 2.3.2 Predicción
```{r}
# Alinear niveles de factores
for (var in cols_factor) {
  levels(test[[var]]) <- levels(train_factors[[var]])
}

# Predicción
pred <- predict(model_forest, newdata = test)

# Resultado final
pred_RF1 <- test %>%
  mutate(price = pred) %>%
  select(property_id, price)

```

```{r}
# Predicciones en entrenamiento
pred_train <- predict(model_forest, newdata = train_factors)

# Calcular MAE en entrenamiento
MAE_train <- mean(abs(train_factors$price - pred_train))
print(paste("MAE en entrenamiento:", round(MAE_train,2)))

```

###2.3.2 Entrenamiento CV Espacial
```{r}
# 1. Preparar base de entrenamiento sin property_id
train_1_RF <- train_factors %>%
  select(-property_id)

# 2. Crear receta
library(recipes)
receta_1_RF <- recipe(price ~ ., data = train_1_RF) %>%
  step_normalize(distancia_cai, distancia_mall, distancia_bus, distancia_avenida) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors())

# 3. Definir modelo de Random Forest
library(parsnip)
modelo_1_RF <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = 500
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

# 4. Crear grid de hiperparámetros
library(dials)
grid_rf <- grid_regular(
  mtry(range = c(2L, 10L)),
  min_n(range = c(2L, 10L)),
  levels = 5
)

# 5. Workflow
library(workflows)
workflow_1_RF <- workflow() %>%
  add_recipe(receta_1_RF) %>%
  add_model(modelo_1_RF)

# 6. Unir con geometría para CV espacial
train_espacial <- train_localidades_sf %>%
  select(property_id, geometry) %>%
  inner_join(train_factors, by = "property_id")

# 7. Validación cruzada espacial
library(spatialsample)
set.seed(123)
block_folds_RF <- spatial_block_cv(train_espacial, v = 5)

# 8. Entrenamiento con CV espacial
library(tune)
resultados_1_RF <- workflow_1_RF %>%
  tune_grid(
    resamples = block_folds_RF,
    grid = grid_rf,
    metrics = metric_set(mae),
    control = control_grid(verbose = TRUE)
  )

# 9. Finalizar y ajustar modelo final
final_workflow_RF <- finalize_workflow(workflow_1_RF, best_params_RF)
final_model_RF <- fit(final_workflow_RF, data = train_1_RF)

# 10. Importancia de variables
library(vip)

# Crear el gráfico 
graf_vip <- vip(extract_fit_parsnip(final_model_RF), num_features = 20)

# Guardar el gráfico 
ggsave("C:/MECA/2025/BIG DATA Y MACHINE LEARNING- Ignasio Sarmiento/Taller3/T3_BDML/04_Views/GRAFICO9-IMPORTANCIA-VARIABLES.png",
       plot = graf_vip, dpi = 150, width = 8, height = 6)

```


```{r}
#Gráfico

library(ggplot2)

# Crear el gráfico y asignarlo a un objeto
graf_mae <- collect_metrics(resultados_1_RF) %>%
  filter(.metric == "mae") %>%
  ggplot(aes(x = mtry, y = mean, color = factor(min_n))) +
  geom_point() +
  geom_line() +
  labs(title = "MAE en validación cruzada espacial",
       subtitle = "Grid de hiperparámetros",
       y = "MAE promedio", x = "mtry", color = "min_n") +
  theme_minimal()

# Guardar el gráfico
ggsave("C:/MECA/2025/BIG DATA Y MACHINE LEARNING- Ignasio Sarmiento/Taller3/T3_BDML/04_Views/GRAFICO10-MAE-VALIDACION-CRUZADA.png",
       plot = graf_mae, dpi = 150, width = 8, height = 6)

```


```{r}
# Crear el gráfico
graf_hist_price <- ggplot(resultados_2_RF, aes(x = price)) +
  geom_histogram(bins = 30, fill = "#0072B2", color = "white") +
  labs(title = "Distribución de precios predichos (Random Forest)",
       x = "Precio predicho", y = "Frecuencia") +
  theme_minimal()

# Guardarlo como imagen
ggsave("C:/MECA/2025/BIG DATA Y MACHINE LEARNING- Ignasio Sarmiento/Taller3/T3_BDML/04_Views/GRAFICO11-HISTOGRAMA-PREDICCIONES.png",
       plot = graf_hist_price, dpi = 150, width = 8, height = 6)


```

```{r}
# Crear el boxplot
graf_boxplot_price <- ggplot(resultados_2_RF, aes(y = price)) +
  geom_boxplot(fill = "#009E73") +
  labs(title = "Boxplot de precios predichos",
       y = "Precio predicho") +
  theme_minimal()

# Guardar el gráfico como imagen
ggsave("C:/MECA/2025/BIG DATA Y MACHINE LEARNING- Ignasio Sarmiento/Taller3/T3_BDML/04_Views/GRAFICO12-BOXPLOT-PREDICCIONES.png",
       plot = graf_boxplot_price, dpi = 150, width = 6, height = 6)

```




### 2.3.3 Predicción
```{r}
# 1. Preparar base test sin property_id
test_1_RF <- test %>%
  select(-property_id)

# 2. Predecir con el modelo final
predicciones_RF <- predict(final_model_RF, new_data = test_1_RF)

# 3. Crear data frame con el formato requerido
resultados_2_RF <- test %>%
  select(property_id) %>%
  mutate(price = predicciones_RF$.pred)

```


### 2.3.3 Template Kaggle

```{r}
# Descargar archivo
write.csv(pred_RF1,
          "C:/MECA/2025/BIG DATA Y MACHINE LEARNING- Ignasio Sarmiento/Taller3/T3_BDML/03_Stores/18-05_G4_RANDOM-FOREST1.csv", row.names = FALSE, 
          quote = FALSE)

write.csv(resultados_2_RF,
          "C:/MECA/2025/BIG DATA Y MACHINE LEARNING- Ignasio Sarmiento/Taller3/T3_BDML/03_Stores/20-05_G4_RANDOM-FOREST2.csv", row.names = FALSE, 
          quote = FALSE)

```


## 2.4 Boosting

### 2.4.1 Entrenamiento CV tradicional
```{r}
# Cargar paquetes
pacman::p_load(caret, xgboost, Metrics)

# Función para usar MAE como métrica en caret
maeSummary <- function(data, lev = NULL, model = NULL) {
  out <- Metrics::mae(data$obs, data$pred)
  names(out) <- "MAE"
  out
}

# Definir grid
grid_xgb <- expand.grid(
  nrounds = c(100, 250),
  max_depth = c(2, 3),
  eta = c(0.1, 0.05),
  gamma = c(0, 1),
  colsample_bytree = c(0.6),
  min_child_weight = c(10),
  subsample = c(0.8)
)

# Control con CV tradicional y MAE como métrica
ctrl_mae <- trainControl(
  method = "cv",
  number = 5,
  summaryFunction = maeSummary
)

# Entrenar
set.seed(123)
modelo_xgb <- train(
  formula_modelo,
  data = train_factors,
  method = "xgbTree",
  trControl = ctrl_mae,
  tuneGrid = grid_xgb,
  metric = "MAE",
  verbosity = 0
)

```

### 2.4.2 Predicción CV tradicional
```{r}
# Igualar niveles de factores entre train y test
for (col in cols_factor) {
  test[[col]] <- factor(test[[col]], levels = levels(train_factors[[col]]))
}

predicciones_xgboost <- predict(modelo_xgb, newdata = test)

# Guardar resultados
resultados_prediccion_boosting <- test_aptos %>%
  mutate(predicted_price_xgboost = predicciones_xgboost)

# Preparar formato de Kaggle
pred_boosting1 <- resultados_prediccion_boosting %>%
  select(property_id, predicted_price_xgboost) %>%
  rename(price = predicted_price_xgboost)


```

### 2.4.3 Entrenamiento CV Espacial
```{r}
# Cargar paquetes
pacman::p_load(caret, xgboost, Metrics)

library(sf)
library(dplyr)

train_espacial <- train_localidades_sf %>%
  select(property_id, geometry) %>%
  inner_join(train_factors, by = "property_id")

library(spatialsample)

set.seed(123)
block_folds_EN <- spatial_block_cv(train_espacial, v = 5)

library(ggplot2)
autoplot(block_folds_EN)

# Definir grid
grid_xgb <- expand.grid(
  nrounds = c(100, 250),
  max_depth = c(2, 3),
  eta = c(0.1, 0.05),
  gamma = c(0, 1),
  colsample_bytree = c(0.6),
  min_child_weight = c(10),
  subsample = c(0.8)
)

library(rsample)

index <- lapply(block_folds_EN$splits, function(split) {
  as.integer(rownames(analysis(split)))
})

indexOut <- lapply(block_folds_EN$splits, function(split) {
  as.integer(rownames(assessment(split)))
})

# Control
ctrl_espacial_mae <- trainControl(
  method = "cv",
  number = 5,
  index = index,
  indexOut = indexOut,
  summaryFunction = maeSummary
)

# Entrenar
set.seed(123)
modelo_xgb_espacial <- train(
  formula_modelo,
  data = train_factors,
  method = "xgbTree",
  trControl = ctrl_espacial_mae,
  tuneGrid = grid_xgb,
  metric = "MAE",
  verbosity = 0
)

```

### 2.4.4 Predicción CV tradicional

```{r}
# Asegurar que las variables categóricas en test tengan los mismos niveles que en train_factors
for (col in cols_factor) {
  test[[col]] <- factor(test[[col]], levels = levels(train_factors[[col]]))
}

# Hacer las predicciones
predicciones_xgboost_espacial <- predict(modelo_xgb_espacial, newdata = test)

# Guardar resultados con predicción
resultados_prediccion_boosting_espacial <- test %>%
  mutate(predicted_price_xgboost_espacial = predicciones_xgboost_espacial)

# Preparar formato para Kaggle (o tu entrega)
pred_boosting2 <- resultados_prediccion_boosting_espacial %>%
  select(property_id, predicted_price_xgboost_espacial) %>%
  rename(price = predicted_price_xgboost_espacial)

```



### 2.4.5 Template Kaggle

```{r}
write.csv(pred_boosting1,
          "C:/Users/njaco/OneDrive/Documentos/Mestría en Economía Aplicada/Semestre 3/Big Data y Machine Learning/Repositorios/T3_BDML/03_Stores/25-05_BOOSTING1.csv", row.names = FALSE, 
          quote = FALSE)

write.csv(pred_boosting2,
          "C:/Users/njaco/OneDrive/Documentos/Mestría en Economía Aplicada/Semestre 3/Big Data y Machine Learning/Repositorios/T3_BDML/03_Stores/25-05_BOOSTING2.csv", row.names = FALSE, 
          quote = FALSE)

```

### 2.4.6 Comparación modelos MAE

```{r}
# Extraer el menor MAE de cada modelo
mae_trad_boosting <- min(modelo_xgb$results$MAE)
mae_esp_boosting <- min(modelo_xgb_espacial$results$MAE)

# Comparar en una tabla
tabla_comparacion_mae <- data.frame(
  Modelo = c("CV Tradicional", "CV Espacial"),
  MAE = c(mae_trad_boosting, mae_esp_boosting)
)

print(tabla_comparacion_mae)

```



## 2.5 Redes neuronales

### 2.5.1 Entrenamiento

Modeo 1
```{r}
p_load( rio, ## read datasets
        tidyverse, # manipular dataframes
        tidymodels, #modelos de ML, colección de 8  librerías
        nnet, # redes neuronales de una sola capa
        workflows,
        yardstick,
        recipes,
        tune
        ) 

# Receta del procesamiento
recipe_nnet <- recipes::recipe(
  formula_modelo  , data = train_factors) %>%
  step_novel(all_nominal_predictors()) %>%   # para las clases no antes vistas en el train. 
  step_dummy(all_nominal_predictors()) %>%  # crea dummies para las variables categóricas
  step_zv(all_predictors()) %>%   #  elimina predictores con varianza cero (constantes)
  step_normalize(all_predictors())  # normaliza los predictores. 

# Definición del modelo con hiperparámetros tuneables
nnet_base <- mlp(
  hidden_units = 10,   
  epochs = 50,        
  penalty = 0.01) %>%
  set_mode("regression") %>%
  set_engine("nnet")

nnet_base

# Flujo de trabajo
workflow_base <- workflow() %>% 
  add_recipe(recipe_nnet) %>%
  add_model(nnet_base) 

# Validación cruzada
set.seed(143)
cv_folds <- vfold_cv(train_factors, v = 5)

# Grilla de hiperparámetros

grid_nnet <- grid_regular(
  hidden_units(range = c(3, 20)),
  epochs(range = c(50, 200)),
  levels = 5
)

# Entrenar con validación cruzada (sin tuning)
resamples_fit <- fit_resamples(
  workflow_base,
  resamples = cv_folds,
  metrics = metric_set(mae, rmse),
  control = control_resamples(save_pred = TRUE)
)

# Ver métricas promedio
collect_metrics(resamples_fit)

# Ajuste final sobre todos los datos
final_fit <- fit(workflow_base, data = train_factors)

```

Modeo 2 - cv tradicional

```{r}
# _______________________________________________________________
# Cargar paquetes
p_load(
  tidyverse,
  tidymodels,
  keras,
  recipes,
  rsample,
  yardstick
)

# Crear receta
receta <- recipe(price ~ ., data = train_factors) %>%
  step_novel(all_nominal_predictors()) %>%
  step_other(all_nominal_predictors(), threshold = 0.01) %>%  # Agrupa categorías poco frecuentes
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())

# Preparar la receta
receta_prep <- prep(receta)


# Obtener datos procesados
x <- bake(receta_prep, new_data = NULL) %>%
  select(-price) %>%
  as.matrix()

y <- bake(receta_prep, new_data = NULL) %>%
  pull(price)

# función para crear modelo con keras
rn_model <- function(input_shape) {
  keras_model_sequential() %>%
    layer_dense(units = 64, activation = "relu", input_shape = input_shape) %>%
    layer_dropout(rate = 0.3) %>%
    layer_dense(units = 32, activation = "relu") %>%
    layer_dense(units = 1)  # salida continua para regresión
}

# Crear folds de validación cruzada
set.seed(123)
folds_rn <- vfold_cv(data.frame(x), v = 5)

# Lista para almacenar resultados
resultados_rn <- list()

for (i in seq_along(folds_rn$splits)) {
  cat("Fold", i, "\n")
  
  split <- folds_rn$splits[[i]]
  
  # Entrenamiento
  x_train <- analysis(split)
  y_train <- y[as.integer(split$in_id)]
  
  # Validación
  x_val <- assessment(split)
  y_val <- y[-as.integer(split$in_id)]
  
  # Crear modelo
  model <- rn_model(input_shape = ncol(x_train))
  
  model %>% compile(
    loss = "mse",
    optimizer = optimizer_rmsprop(),
    metrics = list("mean_absolute_error")
  )
  
  # Entrenar
  history <- model %>% fit(
    x = as.matrix(x_train), y = y_train,
    epochs = 100, batch_size = 32,
    validation_data = list(as.matrix(x_val), y_val),
    verbose = 0
  )
  
  # Evaluar en fold
  scores <- model %>% evaluate(as.matrix(x_val), y_val, verbose = 0)
  resultados_rn[[i]] <- scores  <- scores
}

# Resultados de los folds
t_fold_rn <- map_dfr(resultados_rn, ~tibble(
  mse = .x["loss"],
  mae = .x["mean_absolute_error"]
))
#t_fold_rn <- map_dfr(result_fold_rn, ~as_tibble_row(.x)) %>%
 # rename(mse = loss, mae = mean_absolute_error)

t_fold_rn %>%
  summarise(
    media_mae = mean(mae),
    sd_mae = sd(mae),
    media_mse = mean(mse)
  )

# Entrenar el modelo final
modelo_rn2 <- rn_model(ncol(x))
modelo_rn2 %>% compile(
  loss = "mse",
  optimizer = optimizer_rmsprop(),
  metrics = list("mean_absolute_error")
)

history_final <- modelo_rn2 %>% fit(
  x = x,
  y = y,
  epochs = 50,
  batch_size = 32,
  validation_split = 0.2
)


graf_RD_Tain2 <- plot(history_final)
ggsave("../04_Views/GRAFICO2.5-TRAIN-RD2.png", plot = graf_RD_Tain2, dpi = 150, width = 8, height = 6)

```

modelo 3 - cv espacial

```{r}
library(tidymodels)
library(keras)
library(dplyr)
library(purrr)

# Unir train y test para asignar zonas consistentes
data_geo <- bind_rows(train_factors %>% select(lat, lon),
                      test %>% select(lat, lon))

# K-means con 5 zonas (podés ajustar 'centros')
set.seed(123)
zonas <- kmeans(data_geo, centers = 5)

# Añadir zona al dataset
train_factors$zona <- zonas$cluster[1:nrow(train_factors)]
test$zona <- zonas$cluster[(nrow(train_factors)+1):nrow(data_geo)]

# Crear receta
receta3 <- recipe(price ~ ., data = train_factors) %>%
  step_novel(all_nominal_predictors()) %>%
  step_other(all_nominal_predictors(), threshold = 0.01) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())

# Preparar la receta
receta_prep3 <- prep(receta3)

# Preprocesar datos
procesado3 <- bake(receta_prep3, new_data = NULL)

x3 <- procesado3 %>% select(-price) %>% as.matrix()
y3 <- procesado3$price

# Añadir columna espacial para agrupar (ej. zona)
procesado3$zona <- train_factors$zona  # Asegurate que `zona` esté en tu data

# Crear folds de validación cruzada espacial
set.seed(123)
folds_espaciales3 <- group_vfold_cv(procesado3, group = zona, v = 5)

# Función para crear modelo
rn_model3 <- function(input_shape) {
  keras_model_sequential() %>%
    layer_dense(units = 64, activation = "relu", input_shape = input_shape) %>%
    layer_dropout(rate = 0.3) %>%
    layer_dense(units = 32, activation = "relu") %>%
    layer_dense(units = 1)
}

# Lista para resultados
resultados_espaciales <- list()

for (i in seq_along(folds_espaciales3$splits)) {
  cat("Fold espacial", i, "\n")
  
  split <- folds_espaciales3$splits[[i]]
  
  data_train <- analysis(split)
  data_val <- assessment(split)
  
  x_train <- data_train %>% select(-price, -zona) %>% as.matrix()
  y_train <- data_train$price
  
  x_val <- data_val %>% select(-price, -zona) %>% as.matrix()
  y_val <- data_val$price
  
  # Crear y compilar modelo
  model_rn3 <- rn_model3(ncol(x_train))
  model_rn3 %>% compile(
    loss = "mse",
    optimizer = optimizer_rmsprop(),
    metrics = list("mean_absolute_error")
  )
  
  # Entrenar
  history <- model_rn3 %>% fit(
    x = x_train, y = y_train,
    epochs = 100, batch_size = 32,
    validation_data = list(x_val, y_val),
    verbose = 0
  )
  
  # Evaluar
  scores3 <- model_rn3 %>% evaluate(x_val, y_val, verbose = 0)
  resultados_espaciales[[i]] <- scores3
}

graf_RD_Tain3 <- plot(history)
ggsave("../04_Views/GRAFICO2.5-TRAIN-RD3.png", plot = graf_RD_Tain3, dpi = 150, width = 8, height = 6)

# Tabla de resultados
t_espacial <- map_dfr(resultados_espaciales, ~tibble(
  mse = .x["loss"],
  mae = .x["mean_absolute_error"]
))

# Promedios
t_espacial %>%
  summarise(
    media_mae = mean(mae),
    sd_mae = sd(mae),
    media_mse = mean(mse)
  )
```


modelo 4 - cv espacial (cambio de zonas y log price)

```{r}
library(tidymodels)
library(keras)
library(dplyr)
library(purrr)

# 1. Unir train y test para asignar zonas consistentes
data_geo4 <- bind_rows(train_factors %>% select(lat, lon),
                       test %>% select(lat, lon))

# 2. K-means con 5 zonas
set.seed(123)
zonas4 <- kmeans(data_geo4, centers = 5, nstart = 25)

# 3. Asignar zona a cada set
train_factors$zona4 <- zonas4$cluster[1:nrow(train_factors)]
test$zona4 <- zonas4$cluster[(nrow(train_factors)+1):nrow(data_geo4)]

# 4. Crear receta con log(price)
train_log <- train_factors %>%
  mutate(price = log(price + 1))  # equivalente a step_log(price, offset = 1)


receta4 <- recipe(price ~ ., data = train_log) %>%
  step_novel(all_nominal_predictors()) %>%
  step_other(all_nominal_predictors(), threshold = 0.01) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())

# 5. Preparar receta
receta_prep4 <- prep(receta4)

# 6. Preprocesar datos
procesado4 <- bake(receta_prep4, new_data = NULL)

# 7. Matrices de entrada
x4 <- procesado4 %>% select(-price) %>% as.matrix()
y4 <- procesado4$price

# 8. Añadir columna de zona para agrupamiento
procesado4$zona4 <- train_factors$zona4

# 9. Crear folds espaciales
set.seed(123)
folds_espaciales4 <- group_vfold_cv(procesado4, group = zona4, v = 5)

# 10. Función para crear modelo
rn_model4 <- function(input_shape) {
  keras_model_sequential() %>%
    layer_dense(units = 64, activation = "relu", input_shape = input_shape) %>%
    layer_dropout(rate = 0.3) %>%
    layer_dense(units = 32, activation = "relu") %>%
    layer_dense(units = 1)
}

# 11. Lista para resultados
resultados_espaciales4 <- list()

# 12. Entrenamiento con validación cruzada espacial
for (i in seq_along(folds_espaciales4$splits)) {
  cat("Fold espacial", i, "\n")
  
  split4 <- folds_espaciales4$splits[[i]]
  
  data_train4 <- analysis(split4)
  data_val4 <- assessment(split4)
  
  x_train4 <- data_train4 %>% select(-price, -zona4) %>% as.matrix()
  y_train4 <- data_train4$price
  
  x_val4 <- data_val4 %>% select(-price, -zona4) %>% as.matrix()
  y_val4 <- data_val4$price
  
  # Crear y compilar modelo
  model_rn4 <- rn_model4(ncol(x_train4))
  model_rn4 %>% compile(
    loss = "mse",
    optimizer = optimizer_rmsprop(),
    metrics = list("mean_absolute_error")
  )
  
  # Entrenar
  history4 <- model_rn4 %>% fit(
    x = x_train4, y = y_train4,
    epochs = 100, batch_size = 32,
    validation_data = list(x_val4, y_val4),
    verbose = 0
  )
  
  # Evaluar
  scores4 <- model_rn4 %>% evaluate(x_val4, y_val4, verbose = 0)
  resultados_espaciales4[[i]] <- scores4
}


# 13. Plot del último fold
graf_RD_Tain4 <- plot(history4)

ggsave("../04_Views/GRAFICO2.5-TRAIN-RD4.png", plot = graf_RD_Tain4, dpi = 150, width = 8, height = 6)

# 14. Tabla de resultados
t_espacial4 <- map_dfr(resultados_espaciales4, ~tibble(
  mse = .x["loss"],
  mae = .x["mean_absolute_error"]
))

# 15. Promedios y desviaciones
t_espacial4 %>%
  summarise(
    media_mae = mean(mae),
    sd_mae = sd(mae),
    media_mse = mean(mse)
  )
```

Modelo 5 - cv espacial log price

```{r}
library(tidymodels)
library(keras)
library(dplyr)
library(purrr)

# 1. Unir train y test para asignar zonas consistentes
data_geo5 <- bind_rows(train_factors %>% select(lat, lon),
                       test %>% select(lat, lon))

# 2. K-means con 5 zonas
set.seed(123)
zonas5 <- kmeans(data_geo5, centers = 5, nstart = 25)

# 3. Asignar zona a cada set
train_factors$zona5 <- zonas5$cluster[1:nrow(train_factors)]
test$zona5 <- zonas5$cluster[(nrow(train_factors)+1):nrow(data_geo5)]

# 4. Log-transformar price
train_log5 <- train_factors %>%
  mutate(price = log(price + 1))

# 5. Crear receta
receta5 <- recipe(price ~ ., data = train_log5) %>%
  step_novel(all_nominal_predictors()) %>%
  step_other(all_nominal_predictors(), threshold = 0.01) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())

# 6. Preparar receta
receta_prep5 <- prep(receta5)
procesado5 <- bake(receta_prep5, new_data = NULL)

# 7. Matrices
x5 <- procesado5 %>% select(-price) %>% as.matrix()
y5 <- procesado5$price
procesado5$zona5 <- train_factors$zona5

# 8. Folds espaciales
set.seed(123)
folds_espaciales5 <- group_vfold_cv(procesado5, group = zona5, v = 5)

# 9. Modelo
rn_model5 <- function(input_shape) {
  keras_model_sequential() %>%
    layer_dense(units = 64, activation = "relu", input_shape = input_shape) %>%
    layer_dropout(rate = 0.4) %>%
    layer_dense(units = 32, activation = "relu") %>%
    layer_dense(units = 1)
}

# 10. Entrenamiento con validación cruzada
resultados_espaciales5 <- list()

for (i in seq_along(folds_espaciales5$splits)) {
  cat("Fold espacial", i, "\n")
  
  split5 <- folds_espaciales5$splits[[i]]
  data_train5 <- analysis(split5)
  data_val5 <- assessment(split5)
  
  x_train5 <- data_train5 %>% select(-price, -zona5) %>% as.matrix()
  y_train5 <- data_train5$price
  
  x_val5 <- data_val5 %>% select(-price, -zona5) %>% as.matrix()
  y_val5 <- data_val5$price
  
  model_rn5 <- rn_model5(ncol(x_train5))
  model_rn5 %>% compile(
    loss = "mse",
    optimizer = optimizer_rmsprop(),
    metrics = list("mean_absolute_error")
  )
  
  history5 <- model_rn5 %>% fit(
    x = x_train5, y = y_train5,
    epochs = 30, batch_size = 32,
    validation_data = list(x_val5, y_val5),
    verbose = 0
  )
  
  scores5 <- model_rn5 %>% evaluate(x_val5, y_val5, verbose = 0)
  resultados_espaciales5[[i]] <- scores5
}

# 11. Gráfico
plot(history5)

# 12. Resultados por fold
t_espacial5 <- map_dfr(resultados_espaciales5, ~tibble(
  mse = .x["loss"],
  mae = .x["mean_absolute_error"]
))

# 13. Resumen
t_espacial5 %>%
  summarise(
    media_mae = mean(mae),
    sd_mae = sd(mae),
    media_mse = mean(mse)
  )
```


### 2.5.2 Predicción

modelo 1

```{r}

# Predecir sobre los datos de test (sin price)
pred_rn1 <- predict(final_fit, new_data = test)

resultado_rn1 <- test %>%
  select(property_id) %>%
  bind_cols(data.frame(price = pred_rn1))

# Ver la distribución de las predicciones
graf_RN1 <- ggplot(resultado_rn1, aes(x = .pred/1000000)) +  # Convertir a millones
  geom_histogram(bins = 30, fill = "skyblue", color = "darkblue") +
  labs(title = "Distribución de las predicciones de precio",
       x = "Precio predicho (Millones de Pesos)",
       y = "Frecuencia") +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),  # Centrar título
    axis.text.x = element_text(size = 9)
  ) +
  scale_x_continuous(
    labels = scales::comma_format(accuracy = 0.1),  # Formato de números con comas y 1 decimal
    breaks = scales::pretty_breaks(n = 8)           # Número adecuado de marcas
  )
ggsave("../04_Views/GRAFICO2.5.1-PRED-PRECIO-RD1.png", plot = graf_RN1, dpi = 150, width = 8, height = 6)

# Estadísticas generales de las predicciones
summary(resultado_rn1$.pred)

# Coeficiente de variación de las predicciones totales
cof_var_rn1 <- sd(resultado_rn1$.pred) / mean(resultado_rn1$.pred) * 100
print(paste("Coeficiente de variación de las predicciones (%):", round(cof_var_rn1, 2)))

# Análisis por segmento
seg_rn1 <- test %>%
  select(property_type) %>%
  bind_cols(data.frame(price = pred_rn1)) %>%
  group_by(property_type) %>%
  summarize(
    media = mean(.pred),
    cof_var = round((sd(.pred) / mean(.pred)) * 100, 2)
  )

print(seg_rn1)

# Outliers en predicciones
umbral_superior_rn1 <- quantile(resultado_rn1$.pred, 0.975)
umbral_inferior_rn1 <- quantile(resultado_rn1$.pred, 0.025)

outliers_rn1 <- resultado_rn1 %>%
  filter(.pred > umbral_superior_rn1 | .pred < umbral_inferior_rn1)
print(paste("Número de outliers en predicciones:", nrow(outliers_rn1)))
```

Modelo 2

```{r}
x_test <- bake(receta_prep, new_data = test) %>%
  as.matrix()

# Generar predicciones
pred_rn2 <- modelo_rn2 %>% predict(x_test)

resultado_rn2 <- test %>%
  select(property_id) %>%
  bind_cols(data.frame(price = pred_rn2))

# Ver la distribución de las predicciones
graf_RN2 <- ggplot(resultado_rn2, aes(x = price/1000000)) +  # Convertir a millones
  geom_histogram(bins = 30, fill = "skyblue", color = "darkblue") +
  labs(title = "Distribución de las predicciones de precio",
       x = "Precio predicho (Millones de Pesos)",
       y = "Frecuencia") +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),  # Centrar título
    axis.text.x = element_text(size = 9)
  ) +
  scale_x_continuous(
    labels = scales::comma_format(accuracy = 0.1),  # Formato de números con comas y 1 decimal
    breaks = scales::pretty_breaks(n = 8)           # Número adecuado de marcas
  )

ggsave("../04_Views/GRAFICO2.5.2-PRED-PRECIO-RD2.png", plot = graf_RN2, dpi = 150, width = 8, height = 6)

# Estadísticas generales de las predicciones
summary(resultado_rn2$price)

# Coeficiente de variación de las predicciones totales
cof_var_rn2 <- sd(resultado_rn2$price) / mean(resultado_rn2$price) * 100
print(paste("Coeficiente de variación de las predicciones (%):", round(cof_var_rn2, 2)))

# Análisis por segmento
seg_rn2 <- test %>%
  select(property_type) %>%
  bind_cols(data.frame(price = pred_rn2)) %>%
  group_by(property_type) %>%
  summarize(
    media = mean(price),
    cof_var = round((sd(price) / mean(price)) * 100, 2)
  )

print(seg_rn2)

# Outliers en predicciones
umbral_superior_rn2 <- quantile(resultado_rn2$.pred, 0.975)
umbral_inferior_rn2 <- quantile(resultado_rn2$.pred, 0.025)

outliers_rn2 <- resultado_rn2 %>%
  filter(price > umbral_superior_rn2 | price < umbral_inferior_rn2)
print(paste("Número de outliers en predicciones:", nrow(outliers_rn2)))
```

Modelo 3

```{r}
x_test <- bake(receta_prep, new_data = test) %>%
  as.matrix()

# Generar predicciones
pred_rn3 <- model_rn3 %>% predict(x_test)

resultado_rn3 <- test %>%
  select(property_id) %>%
  bind_cols(data.frame(price = pred_rn3))

# Ver la distribución de las predicciones
graf_RN3 <- ggplot(resultado_rn3, aes(x = price/1000000)) +  # Convertir a millones
  geom_histogram(bins = 30, fill = "skyblue", color = "darkblue") +
  labs(title = "Distribución de las predicciones de precio",
       x = "Precio predicho (Millones de Pesos)",
       y = "Frecuencia") +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),  # Centrar título
    axis.text.x = element_text(size = 9)
  ) +
  scale_x_continuous(
    labels = scales::comma_format(accuracy = 0.1),  # Formato de números con comas y 1 decimal
    breaks = scales::pretty_breaks(n = 8)           # Número adecuado de marcas
  )

ggsave("../04_Views/GRAFICO3.5.2-PRED-PRECIO-RD3.png", plot = graf_RN3, dpi = 150, width = 8, height = 6)

# Estadísticas generales de las predicciones
summary(resultado_rn3$price)

# Coeficiente de variación de las predicciones totales
cof_var_rn3 <- sd(resultado_rn3$price) / mean(resultado_rn3$price) * 100
print(paste("Coeficiente de variación de las predicciones (%):", round(cof_var_rn3, 2)))

# Análisis por segmento
seg_rn3 <- test %>%
  select(property_type) %>%
  bind_cols(data.frame(price = pred_rn3)) %>%
  group_by(property_type) %>%
  summarize(
    media = mean(price),
    cof_var = round((sd(price) / mean(price)) * 100, 2)
  )

print(seg_rn3)

# Outliers en predicciones
umbral_superior_rn3 <- quantile(resultado_rn3$price, 0.975)
umbral_inferior_rn3 <- quantile(resultado_rn3$price, 0.025)

outliers_rn3 <- resultado_rn3 %>%
  filter(price > umbral_superior_rn3 | price < umbral_inferior_rn3)
print(paste("Número de outliers en predicciones:", nrow(outliers_rn3)))
```

Modelo 4 - valuidación cruzada espacial (cambio de zonas)

```{r}
# Preprocesar test con la misma receta
x_test4 <- bake(receta_prep4, new_data = test) %>%
  select(-zona4) %>%
  as.matrix()

# Generar predicciones (en escala log)
pred_log_rn4 <- model_rn4 %>% predict(x_test4)

# Invertir log para obtener precios reales
pred_rn4 <- exp(pred_log_rn4) - 1

# Unir con IDs
resultado_rn4 <- test %>%
  select(property_id) %>%
  bind_cols(data.frame(price = pred_rn4))

# Ver la distribución de las predicciones
graf_RN4 <- ggplot(resultado_rn4, aes(x = price/1000000)) +  # Convertir a millones
  geom_histogram(bins = 30, fill = "skyblue", color = "darkblue") +
  labs(title = "Distribución de las predicciones de precio",
       x = "Precio predicho (Millones de Pesos)",
       y = "Frecuencia") +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),  # Centrar título
    axis.text.x = element_text(size = 9)
  ) +
  scale_x_continuous(
    labels = scales::comma_format(accuracy = 0.1),  # Formato de números con comas y 1 decimal
    breaks = scales::pretty_breaks(n = 8)           # Número adecuado de marcas
  )

ggsave("../04_Views/GRAFICO3.5.2-PRED-PRECIO-RD4.png", plot = graf_RN4, dpi = 150, width = 8, height = 6)

# Estadísticas generales de las predicciones
summary(resultado_rn4$price)

# Coeficiente de variación de las predicciones totales
cof_var_rn4 <- sd(resultado_rn4$price) / mean(resultado_rn4$price) * 100
print(paste("Coeficiente de variación de las predicciones (%):", round(cof_var_rn4, 2)))

# Análisis por tipo de propiedad
seg_rn4 <- test %>%
  select(property_type) %>%
  bind_cols(data.frame(price = pred_rn4)) %>%
  group_by(property_type) %>%
  summarize(
    media = mean(price),
    cof_var = round((sd(price) / mean(price)) * 100, 2)
  )

print(seg_rn4)

# Detección de outliers en predicciones
umbral_superior_rn4 <- quantile(resultado_rn4$price, 0.975)
umbral_inferior_rn4 <- quantile(resultado_rn4$price, 0.025)

outliers_rn4 <- resultado_rn4 %>%
  filter(price > umbral_superior_rn4 | price < umbral_inferior_rn4)

print(paste("Número de outliers en predicciones:", nrow(outliers_rn4)))
```

Modelo 5 - cv espacial, log price y ajuste de hiperparametros para disminuir sobreajuste de modelo 4

```{r}
x_test5 <- bake(receta_prep5, new_data = test) %>%
  select(-zona5) %>%
  as.matrix()

pred_log_rn5 <- model_rn5 %>% predict(x_test5)
pred_rn5 <- exp(pred_log_rn5) - 1

resultado_rn5 <- test %>%
  select(property_id) %>%
  bind_cols(data.frame(price = pred_rn5))

# 15. Gráfico
graf_RN5 <- ggplot(resultado_rn5, aes(x = price / 1e6)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "darkblue") +
  labs(title = "Distribución de las predicciones de precio (Modelo 5)",
       x = "Precio predicho (Millones de Pesos)",
       y = "Frecuencia") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold")) +
  scale_x_continuous(labels = scales::comma_format(accuracy = 0.1),
                     breaks = scales::pretty_breaks(n = 8))

ggsave("../04_Views/GRAFICO3.5.2-PRED-PRECIO-RD5.png", plot = graf_RN5, dpi = 150, width = 8, height = 6)

# 16. Estadísticas
summary(resultado_rn5$price)

# 17. Coeficiente de variación
cv_rn5 <- sd(resultado_rn5$price) / mean(resultado_rn5$price) * 100
print(paste("Coeficiente de variación de las predicciones (%):", round(cv_rn5, 2)))

# 18. Por tipo de propiedad
seg_rn5 <- test %>%
  select(property_type) %>%
  bind_cols(data.frame(price = pred_rn5)) %>%
  group_by(property_type) %>%
  summarise(
    media = mean(price),
    cof_var = round((sd(price) / mean(price)) * 100, 2)
  )
print(seg_rn5)

# 19. Outliers
umbral_sup5 <- quantile(resultado_rn5$price, 0.975)
umbral_inf5 <- quantile(resultado_rn5$price, 0.025)

outliers5 <- resultado_rn5 %>%
  filter(price > umbral_sup5 | price < umbral_inf5)

print(paste("Número de outliers en predicciones:", nrow(outliers5)))
```

### 2.5.3 Template Kaggle

```{r}
# Descargar archivo
write.csv(resultado_rn2,
          "../03_Stores/24-05_G4_RedNeuronal1.csv", row.names = FALSE, 
          quote = FALSE)# cv tradicional

write.csv(resultado_rn3,
          "../03_Stores/24-05_G4_RedNeuronal2.csv", row.names = FALSE, 
          quote = FALSE) # cv espacial
write.csv(resultado_rn5,
          "../03_Stores/24-05_G4_RedNeuronal3.csv", row.names = FALSE, 
          quote = FALSE) # cv espacial, log price y ajuste de hiperparametros para disminuir sobreajuste de modelo 4

write.csv(resultado_rn4,
          "../03_Stores/24-05_G4_RedNeuronal4.csv", row.names = FALSE, 
          quote = FALSE) # cv espacial, log price 
```

## 2.7 Regresión lineal

### 2.5.1 Entrenamiento

Modelo 1: todas la variables sin niguna consideración especial

```{r}

# Definir fórmula del modelo de regresión lineal
variables_lm <- paste(pred_var_lm, collapse = " + ")
formula_lm <- as.formula(paste("price ~", variables_lm))

# Modelos

modelo_lm1 <- lm(formula_lm, data = train_factors)
summary(modelo_lm1)

modelo_lm2 <- lm(price ~ year + rooms + bathrooms  + surface_covered + property_type  + distancia_mall + distancia_bus + distancia_avenida + binaria_parking + binaria_gym  + binaria_elevator, data = train_factors)
summary(modelo_lm2)


library(vip)
importance_lm1 <- vip(modelo_lm1, num_features = 20)
ggsave("../04_Views/GRAFICO2.7.1VAR-IMPORTANCE-LM1.png", plot = importance_lm1, dpi = 150, width = 8, height = 6)

importance_lm2 <- vip(modelo_lm2, num_features = 20)
ggsave("../04_Views/GRAFICO2.7.2VAR-IMPORTANCE-LM2.png", plot = importance_lm2, dpi = 150, width = 8, height = 6)

modelo_lm3 <- lm(price ~  bathrooms  + surface_covered  + distancia_avenida + binaria_elevator +rooms, data = train_factors)
summary(modelo_lm3)

```

Modelo con precio como log

```{r}
# Convertir el precio en log
train_factors_lm_log <- train_factors %>%
  mutate(log_price = log(price)) %>%  # ← primero calculamos log_price
  select(
    property_id,
    price,
    log_price,
    year,
    property_type,
    rooms,
    bedrooms,
    bathrooms,
    surface_total,
    surface_covered,
    distancia_cai,
    distancia_mall,
    distancia_bus,
    distancia_avenida,
    binaria_parking,
    binaria_terrace,
    binaria_gym,
    binaria_laundry,
    binaria_elevator
  ) %>%
  mutate(across(all_of(cols_factor), as.factor)) %>%
  na.omit() %>%
  as.data.frame()

# Formula para log
formula_log <- as.formula(paste("log_price ~", variables_lm))

# Modelos log lin

modelo_log1 <- lm(formula_log, data = train_factors_lm_log)
summary(modelo_log1)

modelo_log2 <- lm(log_price ~ year + rooms + bathrooms  + surface_covered  + distancia_mall + distancia_bus + distancia_avenida + binaria_parking + binaria_gym  + binaria_elevator, data = train_factors_lm_log)
summary(modelo_log2)

```

### 2.5.2 Predicción

Modelo 1: 

```{r}
# Predicción
pred_lm1 <- predict(modelo_lm1, newdata = test)

resultado_lm1 <- test %>%
  select(property_id) %>%
  bind_cols(data.frame(price = pred_lm1))

# Ver la distribución de las predicciones
graf_lm1 <- ggplot(resultado_lm1, aes(x = price/1000000)) +  # Convertir a millones
  geom_histogram(bins = 30, fill = "skyblue", color = "darkblue") +
  labs(title = "Distribución de las predicciones de precio",
       x = "Precio predicho (Millones de Pesos)",
       y = "Frecuencia") +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),  # Centrar título
    axis.text.x = element_text(size = 9)
  ) +
  scale_x_continuous(
    labels = scales::comma_format(accuracy = 0.1),  # Formato de números con comas y 1 decimal
    breaks = scales::pretty_breaks(n = 8)           # Número adecuado de marcas
  )

ggsave("../04_Views/GRAFICO2.7.3PRED-REGLIN1.png", plot = graf_lm1, dpi = 150, width = 8, height = 6)

# Estadísticas generales de las predicciones
summary(resultado_lm1$price)

# Coeficiente de variación de las predicciones totales
cof_var_pred1 <- sd(resultado_lm1$price) / mean(resultado_lm1$price) * 100
print(paste("Coeficiente de variación de las predicciones (%):", round(cof_var_pred1, 2)))

# Análisis por segmento
seg_lm1 <- test %>%
  select(property_type) %>%
  bind_cols(data.frame(price = pred_lm1)) %>%
  group_by(property_type) %>%
  summarize(
    media = mean(price),
    cof_var = round((sd(price) / mean(price)) * 100, 2)
  )

print(seg_lm1)

# Outliers en predicciones
umbral_superior_lm1 <- quantile(resultado_lm1$price, 0.975)
umbral_inferior_lm1 <- quantile(resultado_lm1$price, 0.025)

outliers_lm1 <- resultado_lm1 %>%
  filter(price > umbral_superior_lm1 | price < umbral_inferior_lm1)
print(paste("Número de outliers en predicciones:", nrow(outliers_lm1)))


```

Modelo 2:

```{r}
# Predicción
pred_lm2 <- predict(modelo_lm2, newdata = test)

resultado_lm2 <- test %>%
  select(property_id) %>%
  bind_cols(data.frame(price = pred_lm2))

# Ver la distribución de las predicciones
graf_lm2 <- ggplot(resultado_lm2, aes(x = price/1000000)) +  # Convertir a millones
  geom_histogram(bins = 30, fill = "skyblue", color = "darkblue") +
  labs(title = "Distribución de las predicciones de precio",
       x = "Precio predicho (Millones de Pesos)",
       y = "Frecuencia") +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),  # Centrar título
    axis.text.x = element_text(size = 9)
  ) +
  scale_x_continuous(
    labels = scales::comma_format(accuracy = 0.1),  # Formato de números con comas y 1 decimal
    breaks = scales::pretty_breaks(n = 8)           # Número adecuado de marcas
  )

ggsave("../04_Views/GRAFICO2.7.3PRED-REGLIN1.png", plot = graf_lm2, dpi = 150, width = 8, height = 6)

# Estadísticas generales de las predicciones
summary(resultado_lm2$price)

# Coeficiente de variación de las predicciones totales
cof_var_pred2 <- sd(resultado_lm2$price) / mean(resultado_lm2$price) * 100
print(paste("Coeficiente de variación de las predicciones (%):", round(cof_var_pred2, 2)))

# Análisis por segmento
seg_lm2 <- test %>%
  select(property_type) %>%
  bind_cols(data.frame(price = pred_lm2)) %>%
  group_by(property_type) %>%
  summarize(
    media = mean(price),
    cof_var = round((sd(price) / mean(price)) * 100, 2)
  )

print(seg_lm2)

# Outliers en predicciones
umbral_superior_lm2 <- quantile(resultado_lm2$price, 0.975)
umbral_inferior_lm2 <- quantile(resultado_lm2$price, 0.025)

outliers_lm2 <- resultado_lm2 %>%
  filter(price > umbral_superior_lm2 | price < umbral_inferior_lm2)
print(paste("Número de outliers en predicciones:", nrow(outliers_lm2)))

```

Modeolo 3:

```{r}
# Predicción
pred_lm3 <- predict(modelo_lm3, newdata = test)

resultado_lm3 <- test %>%
  select(property_id) %>%
  bind_cols(data.frame(price = pred_lm3))

# Ver la distribución de las predicciones
graf_pred_lm3 <- ggplot(resultado_lm3, aes(x = price/1000000)) +  # Convertir a millones
  geom_histogram(bins = 30, fill = "skyblue", color = "darkblue") +
  labs(title = "Distribución de las predicciones de precio Reg Lin",
       x = "Precio predicho (Millones de Pesos)",
       y = "Frecuencia") +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),  # Centrar título
    axis.text.x = element_text(size = 9)
  ) +
  scale_x_continuous(
    labels = scales::comma_format(accuracy = 0.1),  # Formato de números con comas y 1 decimal
    breaks = scales::pretty_breaks(n = 8)           # Número adecuado de marcas
  )

# Guardar la gráfica
ggsave("../04_Views/GRAFICO-DISTRIBUCION-PRED-REGLIN-M3.png", plot = graf_pred_lm3,
       dpi = 150, width = 8, height = 6)

# Estadísticas generales de las predicciones
summary(resultado_lm3$price)

# Coeficiente de variación de las predicciones totales
cof_var_pred3 <- sd(resultado_lm3$price) / mean(resultado_lm3$price) * 100
print(paste("Coeficiente de variación de las predicciones (%):", round(cof_var_pred3, 2)))

# Análisis por segmento
seg_lm3 <- test %>%
  select(property_type) %>%
  bind_cols(data.frame(price = pred_lm3)) %>%
  group_by(property_type) %>%
  summarize(
    media = mean(price),
    cof_var = round((sd(price) / mean(price)) * 100, 2)
  )

print(seg_lm3)

# Outliers en predicciones
umbral_superior_lm3 <- quantile(resultado_lm3$price, 0.975)
umbral_inferior_lm3 <- quantile(resultado_lm3$price, 0.025)

outliers_lm3 <- resultado_lm3 %>%
  filter(price > umbral_superior_lm3 | price < umbral_inferior_lm3)
print(paste("Número de outliers en predicciones:", nrow(outliers_lm3)))
```

Modeo 4: mejorar modelo considerando correlación espacial para evitar sobreajuste.

```{r}
# Agregar lat y lon a train_factors_lm 
train_factors_lm <- train_localidades %>%
  select(
    property_id,
    price,
    year,
    property_type,
    rooms,
    bedrooms,
    bathrooms,
    surface_total,
    surface_covered,
    distancia_cai,
    distancia_mall,
    distancia_bus,
    distancia_avenida,
    binaria_parking,
    binaria_terrace,
    binaria_gym,
    binaria_laundry,
    binaria_elevator,
    lat,
    lon  # agregar lat y lon aquí
  ) %>%
  mutate(across(all_of(cols_factor), as.factor)) %>%
  na.omit() %>%
  as.data.frame()

# Variables para el modelo (añadiendo lat y lon)
pred_var_lm_4 <- c(
  'year',
  'rooms',
  'bedrooms',
  'bathrooms',
  'surface_total',
  'surface_covered',
  'property_type',
  'distancia_cai',
  'distancia_mall',
  'distancia_bus',
  'distancia_avenida',
  'binaria_parking',
  'binaria_terrace',
  'binaria_gym',
  'binaria_laundry',
  'binaria_elevator',
  'lat',
  'lon'
)

test_lm_input <- test_aptos %>%
  select(
    property_id,
    year,
    property_type,
    rooms,
    bedrooms,
    bathrooms,
    surface_total,
    surface_covered,
    distancia_cai,
    distancia_mall,
    distancia_bus,
    distancia_avenida,
    binaria_parking,
    binaria_terrace,
    binaria_gym,
    binaria_laundry,
    binaria_elevator, 
    lat,
    lon
  ) %>%
  mutate(across(all_of(cols_factor), as.factor)) %>%
  na.omit() %>%
  as.data.frame()


# Crear fórmula con lat y lon
variables_lm_4 <- paste(pred_var_lm_4, collapse = " + ")
formula_lm_4 <- as.formula(paste("price ~", variables_lm_4))

# Ajustar modelo lineal
modelo_lm4 <- lm(formula_lm_4, data = train_factors_lm)
summary(modelo_lm4)

modelo_lm5 <- lm(price ~  bathrooms  + surface_covered  + distancia_avenida + binaria_elevator + rooms + lat + lon, data = train_factors_lm)
summary(modelo_lm5)


```


Modelos log lin:

```{r}
pred_log1 <- predict(modelo_log1, newdata = test_lm_input)
pred_log1 <- exp(pred_log1)

resultado_log1 <- test_lm_input %>%
  select(property_id) %>%
  bind_cols(data.frame(price = pred_log1))

# Ver la distribución de las predicciones
graf_pred_log1 <- ggplot(resultado_log1, aes(x = price/1000000)) +  # Convertir a millones
  geom_histogram(bins = 30, fill = "skyblue", color = "darkblue") +
  labs(title = "Distribución de las predicciones de precio",
       x = "Precio predicho (Millones de Pesos)",
       y = "Frecuencia") +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),  # Centrar título
    axis.text.x = element_text(size = 9)
  ) +
  scale_x_continuous(
    labels = scales::comma_format(accuracy = 0.1),  # Formato de números con comas y 1 decimal
    breaks = scales::pretty_breaks(n = 8)           # Número adecuado de marcas
  )

# Guardar la gráfica
ggsave("../04_Views/GRAFICO-DISTRIBUCION-PRED-LOGLIN1.png", plot = graf_pred_log1,
       dpi = 150, width = 8, height = 6)

# Estadísticas generales de las predicciones
summary(resultado_log1$price)

# Coeficiente de variación de las predicciones totales
cof_var_log1 <- sd(resultado_log1$price) / mean(resultado_log1$price) * 100
print(paste("Coeficiente de variación de las predicciones (%):", round(cof_var_log1, 2)))

# Análisis por segmento
seg_log1 <- test_lm_input %>%
  select(property_type) %>%
  bind_cols(data.frame(price = pred_log1)) %>%
  group_by(property_type) %>%
  summarize(
    media = mean(price),
    cof_var = round((sd(price) / mean(price)) * 100, 2)
  )

print(seg_log1)

# Outliers en predicciones
umbral_superior_log1<- quantile(resultado_log1$price, 0.975)
umbral_inferior_log1 <- quantile(resultado_log1$price, 0.025)

outliers_log1 <- resultado_log1 %>%
  filter(price > umbral_superior_log1 | price < umbral_inferior_log1)
print(paste("Número de outliers en predicciones:", nrow(outliers_log1)))
```

```{r}
pred_log2 <- predict(modelo_log2, newdata = test_lm_input)
pred_log2 <- exp(pred_log2)

resultado_log2 <- test_lm_input %>%
  select(property_id) %>%
  bind_cols(data.frame(price = pred_log2))

# Ver la distribución de las predicciones
graf_pred_log2 <- ggplot(resultado_log2, aes(x = price/1000000)) +  # Convertir a millones
  geom_histogram(bins = 30, fill = "skyblue", color = "darkblue") +
  labs(title = "Distribución de las predicciones de precio",
       x = "Precio predicho (Millones de Pesos)",
       y = "Frecuencia") +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),  # Centrar título
    axis.text.x = element_text(size = 9)
  ) +
  scale_x_continuous(
    labels = scales::comma_format(accuracy = 0.1),  # Formato de números con comas y 1 decimal
    breaks = scales::pretty_breaks(n = 8)           # Número adecuado de marcas
  )

# Guardar la gráfica
ggsave("../04_Views/GRAFICO-DISTRIBUCION-PRED-LOGLIN1.png", plot = graf_pred_log2,
       dpi = 150, width = 8, height = 6)

# Estadísticas generales de las predicciones
summary(resultado_log2$price)

# Coeficiente de variación de las predicciones totales
cof_var_log2 <- sd(resultado_log2$price) / mean(resultado_log2$price) * 100
print(paste("Coeficiente de variación de las predicciones (%):", round(cof_var_log2, 2)))

# Análisis por segmento
seg_log2 <- test_lm_input %>%
  select(property_type) %>%
  bind_cols(data.frame(price = pred_log2)) %>%
  group_by(property_type) %>%
  summarize(
    media = mean(price),
    cof_var = round((sd(price) / mean(price)) * 100, 2)
  )

print(seg_log2)

# Outliers en predicciones
umbral_superior_log2<- quantile(resultado_log2$price, 0.975)
umbral_inferior_log2 <- quantile(resultado_log2$price, 0.025)

outliers_log2 <- resultado_log2 %>%
  filter(price > umbral_superior_log2 | price < umbral_inferior_log2)
print(paste("Número de outliers en predicciones:", nrow(outliers_log2)))
```


### 2.5.3 Template Kaggle

```{r}
# Descargar archivo
write.csv(resultado_lm3,
          "../03_Stores/18-05_G4_RegLin1.csv", row.names = FALSE, 
          quote = FALSE)

```

## 2.6 Super Learners

### 2.6.1 Entrenamiento CV tradicional

```{r}
library(SuperLearner)
library(xgboost)
library(dplyr)

# Selecciona las variables predictoras (sin property_id ni geometría)
X_train <- train_factors %>% select(-property_id, -price)
Y_train <- train_factors$price

X_test <- test %>% select(-property_id)

# Define la librería de modelos para SuperLearner
sl.lib <- c("SL.lm", "SL.xgboost")

set.seed(123)
fit_SL <- SuperLearner(
  Y = Y_train,
  X = X_train,
  SL.library = sl.lib,
  method = "method.NNLS",
  cvControl = list(V = 5)  # 5-fold CV simple
)

print(fit_SL)

```

### 2.6.2 Predicción CV tradicional

```{r}
# Predicciones en test
pred_sl <- predict(fit_SL, newdata = X_test, onlySL = TRUE)$pred

# Guardar resultados con predicción del SuperLearner
resultados_prediccion_superlearner <- test %>%
  mutate(predicted_price_superlearner = pred_sl)

# Preparar formato para Kaggle 
pred_superlearner <- resultados_prediccion_superlearner %>%
  select(property_id, predicted_price_superlearner) %>%
  rename(price = predicted_price_superlearner)
```

### 2.6.3 Entrenamiento CV espacial

```{r}
library(SuperLearner)
library(spatialsample)
library(dplyr)

# 1. Datos de entrada
X_train <- train_factors %>% select(-property_id, -price)
Y_train <- train_factors$price

# 2. Crear folds espaciales 
train_espacial <- train_localidades_sf %>%
  select(property_id, geometry) %>%
  inner_join(train_factors, by = "property_id")

set.seed(123)
folds_spatial <- spatial_block_cv(train_espacial, v = 5)

# 3. Extraer índices de los folds
index <- lapply(folds_spatial$splits, function(split) {
  analysis(split) %>% pull(property_id)
})

# 4. Convertir índices a posiciones en X_train
index_list <- lapply(index, function(ids) {
  which(train_factors$property_id %in% ids)
})

# 5. Entrenamiento del SuperLearner con folds espaciales
set.seed(123)
fit_SL_espacial <- SuperLearner(
  Y = Y_train,
  X = X_train,
  SL.library = c("SL.lm", "SL.xgboost"),
  method = "method.NNLS",
  cvControl = list(V = 5, validRows = index_list)
)

```

### 2.6.4 Predicción CV espacial

```{r}
# 1. Crear conjunto de predictores
X_test <- test %>% select(-property_id)

# 2. Generar predicciones
pred_espacial <- predict(fit_SL_espacial, newdata = X_test, onlySL = TRUE)$pred

# 3. Armar dataframe final con property_id y price
pred_superlearner_espacial <- test %>%
  select(property_id) %>%
  mutate(price = pred_espacial)
```

### 2.6.5 Template Kaggle

```{r}
# Descargar archivo
write.csv(pred_superlearner,
          "C:/Users/njaco/OneDrive/Documentos/Mestría en Economía Aplicada/Semestre 3/Big Data y Machine Learning/Repositorios/T3_BDML/03_Stores/25-05_SUPERLEARNER1.csv", row.names = FALSE, 
          quote = FALSE)

write.csv(pred_superlearner_espacial,
          "C:/Users/njaco/OneDrive/Documentos/Mestría en Economía Aplicada/Semestre 3/Big Data y Machine Learning/Repositorios/T3_BDML/03_Stores/25-05_SUPERLEARNER2.csv", row.names = FALSE, 
          quote = FALSE)

```

### 2.6.6 Comparación mae

```{r}
mae <- function(actual, predicted) {
  mean(abs(actual - predicted))
}

# Predicciones SuperLearner CV tradicional
pred_train_tradicional <- predict(fit_SL, newdata = X_train, onlySL = TRUE)$pred

# Predicciones SuperLearner CV espacial 
pred_train_espacial <- predict(fit_SL_espacial, newdata = X_train, onlySL = TRUE)$pred

# MAE en entrenamiento
mae_tradicional <- mae(Y_train, pred_train_tradicional)
mae_espacial <- mae(Y_train, pred_train_espacial)

library(tibble)

tibble(
  Modelo = c("SuperLearner CV Tradicional", "SuperLearner CV Espacial"),
  MAE = c(mae_tradicional, mae_espacial)
)
```














