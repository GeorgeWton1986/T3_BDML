---
title: "Taller 3"
author: LAURA SARIF RIVERA SANABRIA,   NICOLAS JACOME VELASCO, JORGE ELIECER VIAFARA
  MORALES, ZAIRA ALEJANDRA GARCIA BERNAL
date: "2025-05-03"
output: html_document
---

# 1. Definición de entorno de trabajo

```{r}
# Instalar y cargar el paquetes
if (!require("pacman")) install.packages("pacman")
library(pacman)

if (!require("MLmetrics")) install.packages("MLmetrics")
library(MLmetrics)

# Usar pacman para cargar (e instalar si es necesario) los paquetes
p_load(tidyverse,   # Manipulación y visualización de datos 
       dplyr,       # Manipulación de datos
       ggplot2,     # Visualización de datos en gráficas
       readr,       # Importación de datos
       stargazer,   # Formato para tablas
       utils,       # Lctura de archivos y manipulación de datos
       skimr,       # Resumen estadístico 
       caret,       # Creación y validación de modelos predictivos 
       glmnet,      # Modelos de regresión penalizados (Ridge, Lasso)
       xgboost,     # Implementación de Gradient Boosting
       rpart,       # Árboles de decisión
       rpart.plot,  # Visualización de árboles generados por rpart
       pROC,        # Curvas ROC y métricas 
       Metrics,     # Métricas de evaluación
       httr,        # Solicitudes http
       rio,         # Facilidad para importar data
       plotly,      # Gráficos interactivos
       osmdata,     # Obtener información de open street maps
       sf,          # Leer/escribir/manipular datos espaciales
       leaflet,     # Mapas interactivos
       gridExtra,   # Graficar en grid
       tmaptools,   # Geocode_OMS()
       geosphere,   # Calcular distancias geográficas como Haversine
       tidymodels,  # Carga recipes, parsnip, rsample, tune, y otros paquetes relacionados
       spatialsample, # Muestreo espacial
       vip
       )
```

## 1.1 Carga base de datos

```{r}
# 1. Carga de datos train 
## Definir URL del archivo Excel en GitHub
url_excel_train <- "https://raw.githubusercontent.com/GeorgeWton1986/T3_BDML/refs/heads/main/03_Stores/train.csv"

## Descargar el archivo temporalmente
temp_file <- tempfile(fileext = ".csv")
GET(url_excel_train, write_disk(temp_file, overwrite = TRUE))

## Leer el archivo CSV en un dataframe
train_aptos <- read_csv(temp_file)


# 2. Carga de datos test hogares
## Definir URL del archivo Excel en GitHub
url_excel_test <- "https://raw.githubusercontent.com/GeorgeWton1986/T3_BDML/refs/heads/main/03_Stores/test.csv"

## Descargar el archivo temporalmente
temp_file <- tempfile(fileext = ".csv")
GET(url_excel_test, write_disk(temp_file, overwrite = TRUE))

# Leer el archivo CSV en un dataframe
test_aptos <- read_csv(temp_file)
```

## 1.2 Inspección y tratamiento BD train

```{r}
# Nombre de las columnas de la base train_hogares
colnames(train_aptos)

# Seleccion de la columnas id de hogares
train_aptos %>%
  select(property_id) %>%
  head()

# Resumen de apartamentos de la base train_hogares
skim(train_aptos)

# Tratamiento de missing values
train_miss <- skim(train_aptos) %>%
  filter(skim_type == "numeric") %>%
  select(skim_variable, n_missing)  

nobs_train <- nrow(train_aptos)

# Porcentaje de missing por cada caracteristica
train_aptos_miss<- train_miss %>% 
  mutate(p_missing= n_missing/nobs_train) %>% 
  filter(n_missing!= 0) %>% 
  arrange(-n_missing)

train_aptos_miss

# Visualizar variables con missing values

## Gráfico para surface_total (área total)
graf_ms1 <- ggplot(train_aptos, aes(surface_total)) +
  geom_histogram(color = "#000000", fill = "#0099F8") +
  geom_vline(xintercept = median(train_aptos$surface_total, na.rm = TRUE), linetype = "dashed", color = "red") +
  geom_vline(xintercept = mean(train_aptos$surface_total, na.rm = TRUE), linetype = "dashed", color = "blue") + 
  ggtitle("GRÁFICA 1 - Área total") +
  theme_bw() +
  theme(plot.title = element_text(size = 18))

ggsave("../04_Views/GRAFICO1-HISTOGRAMA-AREATOTAL.png", plot = graf_ms1, dpi = 150, width = 8, height = 6)

## Gráfico para surface_covered (área privada)
graf_ms2 <- ggplot(train_aptos, aes(surface_covered)) +
  geom_histogram(color = "#000000", fill = "#0099F8") +
  geom_vline(xintercept = median(train_aptos$surface_covered, na.rm = TRUE), linetype = "dashed", color = "red") +
  geom_vline(xintercept = mean(train_aptos$surface_covered, na.rm = TRUE), linetype = "dashed", color = "blue") + 
  ggtitle("GRÁFICA 2 - Área privada") +
  theme_bw() +
  theme(plot.title = element_text(size = 18))

ggsave("../04_Views/GRAFICO2-HISTOGRAMA-AREAPRIVADA.png", plot = graf_ms2, dpi = 150, width = 8, height = 6)

## Gráfico para rooms (habitaciones)
graf_ms3 <- ggplot(train_aptos, aes(rooms)) +
  geom_histogram(color = "#000000", fill = "#0099F8") +
  geom_vline(xintercept = median(train_aptos$rooms, na.rm = TRUE), linetype = "dashed", color = "red") +
  geom_vline(xintercept = mean(train_aptos$rooms, na.rm = TRUE), linetype = "dashed", color = "blue") + 
  ggtitle("GRÁFICA 3 -Habitaciones") +
  theme_bw() +
  theme(plot.title = element_text(size = 18))

ggsave("../04_Views/GRAFICO3-HISTOGRAMA-HABITACIONES.png", plot = graf_ms3, dpi = 150, width = 8, height = 6)

## Gráfico para bathrooms (baños)
graf_ms4 <- ggplot(train_aptos, aes(bathrooms)) +
  geom_histogram(color = "#000000", fill = "#0099F8") +
  geom_vline(xintercept = median(train_aptos$bathrooms, na.rm = TRUE), linetype = "dashed", color = "red") +
  geom_vline(xintercept = mean(train_aptos$bathrooms, na.rm = TRUE), linetype = "dashed", color = "blue") + 
  ggtitle("GRÁFICA 4 - Baños") +
  theme_bw() +
  theme(plot.title = element_text(size = 18))

ggsave("../04_Views/GRAFICO4-HISTOGRAMA-BAÑOS.png", plot = graf_ms4, dpi = 150, width = 8, height = 6)

# Imputación de variables con la mediana y moda

## 1. Variable surface_total con mediana
mediana_surface_total <- median(train_aptos$surface_total, na.rm = TRUE)

train_aptos <- train_aptos  %>%
  mutate(surface_total = ifelse(is.na(surface_total) == TRUE, mediana_surface_total, surface_total))

## 2. Variable surface_covered con mediana
mediana_surface_covered <- median(train_aptos$surface_covered, na.rm = TRUE)

train_aptos <- train_aptos  %>%
  mutate(surface_covered = ifelse(is.na(surface_covered) == TRUE, mediana_surface_covered, surface_covered))

## 3.  Variable rooms con la moda
moda_rooms <- train_aptos %>% 
  filter(!is.na(rooms)) %>% 
  count(rooms) %>% 
  arrange(desc(n)) %>% 
  slice(1) %>% 
  pull(rooms)

train_aptos <- train_aptos  %>%
  mutate(rooms = ifelse(is.na(rooms) == TRUE, moda_rooms, rooms))

## 4.Variable bathrooms con la moda
moda_bathrooms <- train_aptos %>% 
  filter(!is.na(bathrooms)) %>% 
  count(bathrooms) %>% 
  arrange(desc(n)) %>% 
  slice(1) %>% 
  pull(bathrooms)

train_aptos <- train_aptos  %>%
  mutate(bathrooms = ifelse(is.na(bathrooms) == TRUE, moda_bathrooms, bathrooms))

# Validación de correcta imputación de missing values en variables numéricas
skim(train_aptos)
```

Explorar la base para identificar outliers o anomalias de los datos

```{r}
train_aptos <- as.data.frame(train_aptos)
stargazer(train_aptos, type = "text")

# Calcular el precio por metro cuadrado
train_aptos <- train_aptos %>%
  mutate(precio_mt2 = round(price / surface_total, 0))

skim(train_aptos)

# Histograma del precio por metro cuadrado
graf_5 <- ggplot(train_aptos, aes(precio_mt2)) +
  geom_histogram(color = "#000000", fill = "#0099F8") +
  ggtitle("GRÁFICA 4 - Precio por metro cuadrado (millones)") +
  scale_x_continuous(labels = function(x) x / 1e6) +
  theme_bw() +
  theme(plot.title = element_text(size = 18))

ggsave("../04_Views/GRAFICO5-HISTOGRAMA-PRECIOXMETROCUADRADO.png", plot = graf_5, dpi = 150, width = 8, height = 6)

# Box plot del precio por metro cuadrado
graf_6 <- train_aptos %>%
  ggplot(aes(y = precio_mt2)) +
  geom_boxplot(fill = "darkblue", alpha = 0.4) +
  labs(
    title = "Muestra completa",
    y = "Precio por metro cuadrado (millones)", x = "") +
  scale_y_continuous(labels = function(x) x / 1e6) +
  theme_bw()

ggsave("../04_Views/GRAFICO6-BOXPLOT-PRECIOXMETROCUADRADO.png", plot = graf_6, dpi = 150, width = 8, height = 6)

# Ante outliers superiores se aplica la Metodología de percentil 99%
#p1 <- quantile(train_aptos$precio_mt2, probs = 0.01, na.rm = TRUE)
p99 <- quantile(train_aptos$precio_mt2, probs = 0.99, na.rm = TRUE)

train_aptos <- train_aptos %>%
  filter(precio_mt2 <= p99)

# Filtrar solo los valores menores o iguales al límite superior
graf_7 <- train_aptos %>%
  ggplot(aes(y = precio_mt2)) +
  geom_boxplot(fill = "darkblue", alpha = 0.4) +
  labs(
    title = "Muestra filtrada - P99",
    y = "Precio por metro cuadrado (millones)", x = "") +
  scale_y_continuous(labels = function(x) x / 1e6) +
  theme_bw()

ggsave("../04_Views/GRAFICO6-BOXPLOT-PRECIOXMETROCUADRADO-P99.png", plot = graf_7, dpi = 150, width = 8, height = 6)

```

Identificar en el mapa la ubicacion de los apartamentos y casas

```{r}
# Observamos la primera visualización de la ubicación de los inmubles de la base train_aptos
leaflet() %>%
  addTiles() %>%
  addCircles(lng = train_aptos$lon, 
             lat = train_aptos$lat)
```

Carga de datos espaciales
Identificar localidades sin Chapinero

```{r}
# Columas númericas
train_aptos <- train_aptos %>%
  mutate(lon = as.numeric(lon), lat = as.numeric(lat))

limite_chapinero <- getbb("Chapinero, Bogotá, Colombia")


# Las coordenadas en EPSG:4326
train_aptos_sf <- st_as_sf(train_aptos, 
                           coords = c("lon", "lat"), 
                           crs = 4326)

# Filtrar límites sin chapinero
train_localidades <- train_aptos %>%
  filter(
    lon < limite_chapinero[1, "min"] | lon > limite_chapinero[1, "max"] |
    lat < limite_chapinero[2, "min"] | lat > limite_chapinero[2, "max"]
  )

#Train_localicadades NO incluye chapinero
train_localidades_sf <- st_as_sf(train_localidades, 
                                 coords = c("lon", "lat"), 
                                 crs = 4326)

# Visualizar
leaflet() %>%
  addTiles() %>%
  addCircles(lng = train_localidades$lon, 
             lat = train_localidades$lat)
```

Variables en descripción; Tokenizar

```{r}
library(tidytext)
library(dplyr)

# Tokenización de la columna 'description'
train_localidades_tokens <- train_localidades %>%
  unnest_tokens(word, description)
#Quitar stop words
data("stop_words")
train_localidades_tokens_clean <- train_localidades_tokens %>%
  anti_join(stop_words)

```

Palabras más comunes

```{r}
train_localidades_tokens_clean %>%
  count(word, sort = TRUE)
```

```{r}
#train_localidades_tokens_clean %>%
  #count(property_id, word) %>%
  #spread(key = word, value = n, fill = 0)
```

Creación variables externas desde la descripción

```{r}
library(dplyr)

#Vector palabras agrupadas
parking <- c("parqueadero", "garaje", "parqueaderos", "garajes")
terrace <- c("terraza", "balcon")
gym <- "gimnasio"
laundry <- "lavanderia"
elevator <- "ascensor"

#Variable dummy para cada caracteristica
dummies_df <- train_localidades_tokens_clean %>%
  mutate(binaria_parking = word %in% parking,
         binaria_terrace = word %in% terrace,
         binaria_gym = word %in% gym,
         binaria_laundry = word %in% laundry,
         binaria_elevator = word %in% elevator) %>%
  group_by(property_id) %>%
  summarise(across(starts_with("binaria_"), ~as.integer(any(.))))

# Unir a df 
train_localidades <- left_join(train_localidades, dummies_df, by = "property_id")

```

1)  Identificar distancia a Mall(revisarlo porque no carga)

```{r}
# Obtener los límites de Bogotá
limites_bogota <- getbb("Bogotá, Colombia")

mall_bogota <- opq(bbox = limites_bogota) %>%
  add_osm_feature(key = "shop", 
                  value = "mall")

# Descargar datos de Mall
mall_data_bogota <- osmdata_sf(mall_bogota)

# Extraer los puntos de los parques en Bogotá como un objeto sf
mall_sf_bogota <- mall_data_bogota$osm_points %>%
  dplyr::select(osm_id, name) %>%
  st_transform(crs = 4326)  # Asegurarse de que el CRS coincida con el de los apartamentos

# Revisar los datos de los parques
head(mall_sf_bogota)

#¡¡¡IMPORTANTE!!!!
#(MAGNA-SIRGAS / Colombia Bogota)

# Localidades - Transformar a un sistema de coordenadas proyectado para Colombia 
train_localidades_metros <- st_transform(train_localidades_sf,
                               crs = 3116)
print(train_localidades_metros)

# Parques - Transformar a un sistema de coordenadas proyectado para Colombia 
train_mall_metros <- st_transform(mall_sf_bogota,
                               crs = 3116)
print(train_mall_metros)

# Calcular las distancias entre los apartamentos y los parques
dist_matrix_mall <- st_distance(train_localidades_metros, train_mall_metros)

# Calcular la distancia mínima a un parque para cada apartamento
dist_min_mall <- apply(dist_matrix_mall, 1, min)

# Agregar la distancia mínima a la base de datos train_localidades
train_localidades_metros$distancia_mall <- as.numeric(dist_min_mall)

# Agregar la distancia mínima a la base de datos
train_localidades <- train_localidades %>%
  mutate(distancia_mall = dist_min_mall)

# Revisar resultados
head(train_localidades)


```

Visualizar gráfico

```{r}

# Crear histograma
p2 <- ggplot(train_localidades, aes(x = distancia_mall)) +
  geom_histogram(bins = 50, fill = "darkgreen", alpha = 0.5) +
  labs(x = "Distancia mínima hasta el Mall(m)", y = "Cantidad",
       title = "Distribución de distancia a Mall - resto de Bogotá") +
  theme_bw()

ggsave("../04_Views/GRAFICO8B-DISTANCIAMALL_RESTO.png", plot = p2, dpi = 150, width = 8, height = 6)

```

2)  Identificar distancia a CAI

```{r}

# Obtener los CAIs (estaciones de policía) en Bogotá desde OSM
cai_bogota <- opq(bbox = limites_bogota) %>%
  add_osm_feature(key = "amenity", value = "police")

# Descargar datos de Cai
cai_data_bogota <- osmdata_sf(cai_bogota)
# Extraer los puntos de los parques en Bogotá como un objeto sf
cai_sf_bogota <- cai_data_bogota$osm_points %>%
  dplyr::select(osm_id, name) %>%
  st_transform(crs = 4326)  # Asegurarse de que el CRS coincida con el de los apartamentos

# Revisar los datos de los CAI's

head(cai_sf_bogota)
#¡¡¡IMPORTANTE!!!!
#(MAGNA-SIRGAS / Colombia Bogota)
# Localidades - Transformar a un sistema de coordenadas proyectado para Colombia 
train_localidades_metros <- st_transform(train_localidades_sf,
                               crs = 3116)
print(train_localidades_metros)
# Parques - Transformar a un sistema de coordenadas proyectado para Colombia 
train_cai_metros <- st_transform(cai_sf_bogota,
                               crs = 3116)
print(train_cai_metros)
# Calcular las distancias entre los apartamentos y los parques
dist_matrix_cai <- st_distance(train_localidades_metros, train_cai_metros)
# Calcular la distancia mínima a un parque para cada apartamento
dist_min_cai <- apply(dist_matrix_cai, 1, min)
# Agregar la distancia mínima a la base de datos train_localidades
train_localidades_metros$distancia_cai <- as.numeric(dist_min_cai)
# Agregar la distancia mínima a la base de datos
train_localidades <- train_localidades %>%
  mutate(distancia_cai = dist_min_cai)
# Revisar resultados
head(train_localidades)

```
Gráfico de distancia a los CAIS

```{r}
# Visualizar las distancias a los CAI
ggplot(train_localidades, aes(x = distancia_cai)) +
  geom_histogram(bins = 50, fill = "darkred", alpha = 0.5) +
  labs(x = "Distancia mínima a un CAI (m)", y = "Cantidad",
       title = "Distribución de la distancia a los CAI en el resto de Bogotá") +
  theme_bw()

# Guardar el gráfico
ggsave("../04_Views/GRAFICO8C-DISTANCIACAI_RESTO.png", dpi = 150, width = 8, height = 6)
```
3)  Identificar distancia hasta los paraderos de buses

```{r}
# 1. Obtener las paradas de bus en Bogotá desde OSM
bus_stop_bogota <- opq(bbox = limites_bogota) %>%
  add_osm_feature(key = "highway", value = "bus_stop")

# 2. Descargar los datos
bus_stop_data <- osmdata_sf(bus_stop_bogota)

# 3. Extraer los puntos como objeto sf
bus_stop_sf <- bus_stop_data$osm_points %>%
  dplyr::select(osm_id, name) %>%
  st_transform(crs = 4326)

# 4. Transformar las capas al sistema de coordenadas proyectado (MAGNA-SIRGAS)
train_localidades_metros <- st_transform(train_localidades_sf, crs = 3116)
bus_stop_metros <- st_transform(bus_stop_sf, crs = 3116)

# 5. Calcular matriz de distancias entre localidades y paradas de bus
dist_matrix_bus <- st_distance(train_localidades_metros, bus_stop_metros)

# 6. Calcular la distancia mínima a una parada de bus
dist_min_bus <- apply(dist_matrix_bus, 1, min)

# 7. Agregar a la base de datos
train_localidades_metros$distancia_bus <- as.numeric(dist_min_bus)

# 8. Agregar a train_localidades (si aún trabajas con versión en CRS 4326)
train_localidades <- train_localidades %>%
  mutate(distancia_bus = dist_min_bus)

# 9. Revisar resultados
head(train_localidades)

```


Visualizar gráfico

```{r}
library(scales)

ggplot(train_localidades, aes(x = distancia_bus)) +
  geom_histogram(bins = 50, fill = "steelblue", alpha = 0.6) +
  labs(x = "Distancia mínima a una parada de bus (m)", y = "Cantidad",
       title = "Distribución de la distancia a paradas de bus en el resto de Bogotá") +
  scale_x_continuous(labels = comma) +
  theme_bw()

# Guardar gráfico
ggsave("../04_Views/GRAFICO8D-DISTANCIABUS_RESTO.png", dpi = 150, width = 8, height = 6)

```

4)  Identificar distancia a avenidas principales

```{r}

# 1. Obtener avenidas principales en Bogotá desde OSM
avenidas_bogota <- opq(bbox = limites_bogota) %>%
  add_osm_feature(key = "highway", 
                  value = "primary")

# 2. Descargar los datos
avenidas_data <- osmdata_sf(avenidas_bogota)

# 3. Extraer las líneas (vías suelen estar como `osm_lines`)
avenidas_sf <- avenidas_data$osm_lines %>%
  dplyr::select(osm_id, name, highway) %>%
  st_transform(crs = 4326)

# 4. Transformar al sistema proyectado para cálculo de distancias
avenidas_metros <- st_transform(avenidas_sf, crs = 3116)
train_localidades_metros <- st_transform(train_localidades_sf, crs = 3116)

# 5. Calcular la distancia de cada punto a la vía más cercana
dist_matrix_avenidas <- st_distance(train_localidades_metros, avenidas_metros)

# 6. Distancia mínima a una avenida para cada punto
dist_min_avenida <- apply(dist_matrix_avenidas, 1, min)

# 7. Agregar a la base de datos
train_localidades_metros$distancia_avenida <- as.numeric(dist_min_avenida)

train_localidades <- train_localidades %>%
  mutate(distancia_avenida = dist_min_avenida)

# 8. Revisar resultados
head(train_localidades)
```

```{r}
#1. Verificar y asegurar que sea un objeto espacial con CRS 3116
if(!inherits(train_localidades, "sf")) {
  train_localidades_sf <- st_as_sf(
    train_localidades,
    coords = c("lon", "lat"),
    crs = 3116
  )
} else {
  train_localidades_sf <- train_localidades
}

#2. Verificar el CRS este correcto
print(st_crs(train_localidades_sf))
```


Visualizar gráfico

```{r}
ggplot(train_localidades, aes(x = distancia_avenida)) +
  geom_histogram(bins = 50, fill = "darkblue", alpha = 0.6) +
  labs(x = "Distancia mínima a una avenida principal (m)", y = "Cantidad",
       title = "Distribución de la distancia a avenidas principales") +
  theme_minimal()

# Guardar gráfico 
ggsave("../04_Views/GRAFICO8D-DISTANCIAAVENIDA_RESTO.png", dpi = 150, width = 8, height = 6)
```

## 1.3 Inspección y tratamiento de BD test

```{r}
# Nombre de las columnas de la base train_hogares
colnames(test_aptos)

# Seleccion de la columnas id de hogares
test_aptos %>%
  select(property_id) %>%
  head()

# Resumen de apartamentos de la base train_hogares
skim(test_aptos)

# Imputación de missing values con la media de train_aptos

## 1. Variable surface_total con mediana de train
test_aptos <- test_aptos  %>%
  mutate(surface_total = ifelse(is.na(surface_total) == TRUE, mediana_surface_total, surface_total))

## 2. Variable surface_covered con mediana de train
test_aptos <- test_aptos  %>%
  mutate(surface_covered = ifelse(is.na(surface_covered) == TRUE, mediana_surface_covered, surface_covered))

## 3.  Variable rooms con la moda de train
test_aptos <- test_aptos  %>%
  mutate(rooms = ifelse(is.na(rooms) == TRUE, moda_rooms, rooms))

## 4.Variable bathrooms con moda de train
test_aptos <- test_aptos  %>%
  mutate(bathrooms = ifelse(is.na(bathrooms) == TRUE, moda_bathrooms, bathrooms))

# Validación de correcta imputación de missing values en test
skim(test_aptos)
```

Seleccionar las viviendas de la localidad de Chapinero para Test 
```{r}
# Asegurarte de que lon y lat sean numéricos
test_aptos <- test_aptos %>%
  mutate(lon = as.numeric(lon), lat = as.numeric(lat))

# Obtener límites de Chapinero
limite_chapinero <- getbb("Chapinero, Bogotá, Colombia")

# Filtrar SOLO los que están dentro de Chapinero
test_chapinero <- test_aptos %>%
  filter(
    between(lon, limite_chapinero[1, "min"], limite_chapinero[1, "max"]) & 
    between(lat, limite_chapinero[2, "min"], limite_chapinero[2, "max"])
  )

# Crear objeto sf (opcional, si vas a usarlo como spatial)
test_chapinero_sf <- st_as_sf(test_chapinero, 
                              coords = c("lon", "lat"), 
                              crs = 4326)

# Visualizar en el mapa
leaflet() %>%
  addTiles() %>%
  addCircles(lng = test_chapinero$lon, 
             lat = test_chapinero$lat)


```

1)  Distancia a centros comerciales

```{r}
# Asegúrate de que test_chapinero_sf esté definido
test_chapinero_sf <- st_as_sf(test_chapinero,
                              coords = c("lon", "lat"),
                              crs = 4326)

# Transformar a sistema proyectado colombiano (MAGNA-SIRGAS)
test_chapinero_metros <- st_transform(test_chapinero_sf, crs = 3116)

# Transformar también los malls si no lo has hecho aún (puedes omitir si ya existe)
mall_sf_bogota_metros <- st_transform(mall_sf_bogota, crs = 3116)

# Calcular matriz de distancias
dist_matrix_mall_test <- st_distance(test_chapinero_metros, mall_sf_bogota_metros)

# Calcular la distancia mínima a un mall para cada apartamento en test
dist_min_mall_test <- apply(dist_matrix_mall_test, 1, min)

# Agregar la distancia mínima a la base de datos test_chapinero
test_chapinero_metros$distancia_mall <- as.numeric(dist_min_mall_test)

# Agregar también la distancia como columna simple (opcional, para df sin geometría)
test_chapinero <- test_chapinero %>%
  mutate(distancia_mall = dist_min_mall_test)

# Revisar resultados
head(test_chapinero)


```

2) Distancia a CAI

```{r}
# Transformar test_chapinero a objeto sf si aún no está
test_chapinero_sf <- st_as_sf(test_chapinero,
                              coords = c("lon", "lat"),
                              crs = 4326)

# Transformar a sistema de coordenadas proyectado (MAGNA-SIRGAS)
test_chapinero_metros <- st_transform(test_chapinero_sf, crs = 3116)

# Transformar los CAIs a CRS 3116 si no lo hiciste antes
cai_sf_bogota_metros <- st_transform(cai_sf_bogota, crs = 3116)

# Calcular la matriz de distancias
dist_matrix_cai_test <- st_distance(test_chapinero_metros, cai_sf_bogota_metros)

# Calcular la distancia mínima al CAI más cercano para cada apartamento
dist_min_cai_test <- apply(dist_matrix_cai_test, 1, min)

# Agregar la distancia mínima como nueva columna
test_chapinero_metros$distancia_cai <- as.numeric(dist_min_cai_test)

# Agregar la distancia también al data frame base (sin geometría)
test_chapinero <- test_chapinero %>%
  mutate(distancia_cai = dist_min_cai_test)

# Verificar
head(test_chapinero)


```

3)  Identificar distancia hasta los paraderos de buses

```{r}
# Asegúrate de tener test_chapinero_sf como objeto sf
test_chapinero_sf <- st_as_sf(test_chapinero,
                              coords = c("lon", "lat"),
                              crs = 4326)

# Transformar a sistema de coordenadas proyectado (MAGNA-SIRGAS / Bogotá)
test_chapinero_metros <- st_transform(test_chapinero_sf, crs = 3116)

# Asegúrate de tener las paradas de bus transformadas a CRS 3116
bus_stop_metros <- st_transform(bus_stop_sf, crs = 3116)

# Calcular la matriz de distancias entre apartamentos y paradas de bus
dist_matrix_bus_test <- st_distance(test_chapinero_metros, bus_stop_metros)

# Calcular la distancia mínima a una parada de bus
dist_min_bus_test <- apply(dist_matrix_bus_test, 1, min)

# Agregar la distancia mínima a test_chapinero_metros
test_chapinero_metros$distancia_bus <- as.numeric(dist_min_bus_test)

# Agregar la distancia a la versión sin geometría
test_chapinero <- test_chapinero %>%
  mutate(distancia_bus = dist_min_bus_test)

# Verificar
head(test_chapinero)

```

4)  Identificar distancia a avenidas principales

```{r}
# Asegúrate de que test_chapinero_sf tenga geometría (EPSG:4326)
test_chapinero_sf <- st_as_sf(test_chapinero,
                              coords = c("lon", "lat"),
                              crs = 4326)

# 1. Transformar los datos al sistema de coordenadas proyectado (MAGNA-SIRGAS)
test_chapinero_metros <- st_transform(test_chapinero_sf, crs = 3116)

# 2. Asegúrate de tener avenidas_metros definido previamente
# (Ya creado en tu código anterior como st_transform(avenidas_sf, crs = 3116))

# 3. Calcular la matriz de distancias entre apartamentos y avenidas principales
dist_matrix_avenidas_test <- st_distance(test_chapinero_metros, avenidas_metros)

# 4. Calcular la distancia mínima a una avenida
dist_min_avenida_test <- apply(dist_matrix_avenidas_test, 1, min)

# 5. Agregar la distancia mínima a test_chapinero_metros
test_chapinero_metros$distancia_avenida <- as.numeric(dist_min_avenida_test)

# 6. Agregar también a la base de datos test_chapinero sin geometría
test_chapinero <- test_chapinero %>%
  mutate(distancia_avenida = dist_min_avenida_test)

# 7. Verificar resultados
head(test_chapinero)

```

Variables en descripción; Tokenizar

```{r}
library(tidytext)
library(dplyr)

# Tokenización de la columna 'description'
test_chapinero_tokens <- test_chapinero %>%
  unnest_tokens(word, description)

# Quitar stop words
data("stop_words")
test_chapinero_tokens_clean <- test_chapinero_tokens %>%
  anti_join(stop_words)

```

Palabras más comunes

```{r}
test_chapinero_tokens_clean %>%
  count(word, sort = TRUE)
```

Creación variables externas desde la descripción

```{r}

library(dplyr)

#Vector palabras agrupadas
parking <- c("parqueadero", "garaje", "parqueaderos", "garajes")
terrace <- c("terraza", "balcon")
gym <- "gimnasio"
laundry <- "lavanderia"
elevator <- "ascensor"

#Variable dummy para cada caracteristica
dummies_df_test <- test_chapinero_tokens_clean %>%
  mutate(binaria_parking = word %in% parking,
         binaria_terrace = word %in% terrace,
         binaria_gym = word %in% gym,
         binaria_laundry = word %in% laundry,
         binaria_elevator = word %in% elevator) %>%
  group_by(property_id) %>%
  summarise(across(starts_with("binaria_"), ~as.integer(any(.))))

# Unir a df 
test_chapinero <- left_join(test_chapinero, dummies_df_test, by = "property_id")
```

## 1.4 Convertir a factores base entrenamiento
```{r}
# Variables categóricas a convertir en factor
cols_factor <- c(
  'year',
  'property_type',
  'binaria_parking',
  'binaria_terrace',
  'binaria_gym',
  'binaria_laundry',
  'binaria_elevator'
)

# Filtrar y preparar datos
train_factors <- train_localidades %>%
  select(
    property_id,
    price,
    year,
    property_type,
    distancia_cai,
    distancia_mall,
    distancia_bus,
    distancia_avenida,
    binaria_parking,
    binaria_terrace,
    binaria_gym,
    binaria_laundry,
    binaria_elevator
  ) %>%
  mutate(across(all_of(cols_factor), as.factor)) %>%
  as.data.frame()

# Quitar NAs del dataframe preparado
train_factors <- na.omit(train_factors) ## Esta es la base que se usará para el entrenamiento

# Variables predictoras
variables_predictoras <- c(
  'year',
  'property_type',
  'distancia_cai',
  'distancia_mall',
  'distancia_bus',
  'distancia_avenida',
  'precio_mt2',
  'binaria_parking',
  'binaria_terrace',
  'binaria_gym',
  'binaria_laundry',
  'binaria_elevator'
)

# Definir fórmula del modelo
variables <- paste(variables_predictoras, collapse = " + ")
formula_modelo <- as.formula(paste("price ~", variables))

```

Convertir a factores base test
```{r}
# Las columnas estén en el mismo formato que en entrenamiento
cols_factor <- c(
  'year',
  'property_type',
  'binaria_parking',
  'binaria_terrace',
  'binaria_gym',
  'binaria_laundry',
  'binaria_elevator'
)

# Preprocesamiento test
test <- test_chapinero %>%
  select(
    property_id,
    year,
    property_type,
    distancia_cai,
    distancia_mall,
    distancia_bus,
    distancia_avenida,
    binaria_parking,
    binaria_terrace,
    binaria_gym,
    binaria_laundry,
    binaria_elevator
  ) %>%
  mutate(across(all_of(cols_factor), as.factor)) %>%
  as.data.frame()
```

# 2. Entrenamiento de modelos

## 2.1 Elastic net

### 2.1.1 Entrenamiento

```{r}
#1. Eliminar variables innecesarias
train_1_EN <- train_factors %>%
  select(-property_id)

#2. Crear la receta del entranamiento
# Cargar explícitamente el paquete recipes
receta_1_EN <- recipe(price ~ ., data = train_1_EN) %>%
  step_normalize(distancia_cai, distancia_mall, distancia_bus, distancia_avenida) %>% #normalizar solo predictores numéricos (distancias)
  step_dummy(all_nominal_predictors()) %>% ## Convertir variables categóricas a dummies
  step_zv(all_predictors()) #Remover predictores con varianza cero

#3. Definir el modelo
modelo_1_EN <- linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet") 

#4. Especificar los paramatros alpha y lambda EN
#Estamos creando la cuadricula de búsqueda para los hiperparámetros del modelo Elastric Net
#Penalty: lambda que controla la regularización
#Mixture: alpha es el balance entre la regularizacion L1 (Lasso) y L2 (Ridge)

grid_elastic <- grid_regular(
  penalty(range = c(-3, 0), trans = log10_trans()),  # Explorando valores desde 0.001 hasta 1 en escala logaritmica
  mixture(range = c(0, 1)),                          # Explorar de 0 a 1
  levels = c(10, 5)                                  # 10 niveles de penalty, 5 de mixture
)

#5. Crear un workflow (flujo de trabajo)

workflow_1_EN <- workflow() %>%
  add_recipe(receta_1_EN) %>%
  add_model(modelo_1_EN)

#6. Unir train_factors con train_localidades_sf por medio de property_id

train_espacial <- train_localidades_sf %>%
  select(property_id, geometry) %>%
  inner_join(train_factors, by = "property_id")

#7. Estimacion de validacion cruzada espacial por bloques
#Este metodo de VC por bloques permite:
  #1. Mantener la independencia espacial entre los bloques
  #2. Evitar el sobreajuste o estimaciones demasiado optimistas del rendimiento del modelo
library(spatialsample)
set.seed(123)
block_folds_EN <- spatial_block_cv(train_espacial, v = 5)

#8. Verificación visual de los bloques de la valiación cruzada
autoplot(block_folds_EN)

#9. Incluir la libreria purrr
#Vamos a ver el proceso de la validacion cruzada 
library(purrr)
walk(block_folds_EN$splits, function(x) print(autoplot(x)))

#10. Entrenamiento del modelo con validacion cruzada espacial
resultados_1_EN <- workflow_1_EN %>%
  tune_grid(
    resamples = block_folds_EN,
    grid = grid_elastic,
    metrics = metric_set(mae),
    control = control_grid(verbose = TRUE)
  )
#11. Ver los resultados del modelo 1
autoplot(resultados_1_EN)

#12. Resultado del modelo 1
collect_metrics(resultados_1_EN)

#13. Mostrar los mejores resultados
show_best(resultados_1_EN, metric = "mae", n = 5)

#14. Seleccionar los mejores hiperparámetros
best_params <- select_best(resultados_1_EN, metric = "mae")
print(best_params)

#15. Finalizar el modelo con los mejores parámetros
#Se realiza la implementación de los mejores parámetros en el workflow
#¡¡¡IMPORTANTE!!!
#La función finalize_workflow() sustituye los placeholders tune() por los valores específicos de best_params.
#Ademas, reemplaza penalty = tune() y mixture = tune() por los valores numéricos que resultaron óptimos según el criterio de MAE.
#Por lo tanto, final_workflow estan todos los parámetros especificados con valores estimados, listo para ser entrenado en el conjunto completo de datos.
final_workflow <- workflow_1_EN %>%
  finalize_workflow(best_params)

#16. Entrenar el modelo final con todos los datos
final_model <- fit(final_workflow, data = train_1_EN)

#17. Analizar los coeficientes del modelo
# Ver todos los coeficientes (ordenados por magnitud)
library(broom)
coeficientes <- tidy(final_model) %>%
  filter(term != "(Intercept)") %>%
  arrange(desc(abs(estimate)))

#18. Ver los coeficientes más importantes
head(coeficientes, 10)

#19. Ver la importancia de las variables
library(vip)
vip(extract_fit_parsnip(final_model), num_features = 20)

#20. Calcular el coeficiente de variación de los precios reales
cv_precios_reales <- sd(train_1_EN$price) / mean(train_1_EN$price) * 100
print(paste("CV de los precios reales (%):", round(cv_precios_reales, 2)))

```

### 2.1.2 Predicción

```{r}
#1. Tomar la base de test y prepararla para la predicción
na_conteo_test <- colSums(is.na(test))
print(na_conteo_test)

test_EN <- test %>%
  na.omit()

str(test_EN)
print(dim(test_EN))

#2. Realizar predicciones en el conjunto de prueba
prediccion_EN <- predict(final_model, new_data = test_EN)

resultado_EN_1 <- test_EN %>%
  select(property_id) %>%
  bind_cols(prediccion_EN)

head(resultado_EN_1)

#3. Ver la distribución de las predicciones
ggplot(prediccion_EN, aes(x = .pred/1000000)) +  # Convertir a millones
  geom_histogram(bins = 30, fill = "skyblue", color = "darkblue") +
  labs(title = "Distribución de las predicciones de precio",
       x = "Precio predicho (Millones de Pesos)",
       y = "Frecuencia") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),  # Centrar título
    axis.text.x = element_text(size = 9)
  ) +
  scale_x_continuous(
    labels = scales::comma_format(accuracy = 0.1),  # Formato de números con comas y 1 decimal
    breaks = scales::pretty_breaks(n = 8)           # Número adecuado de marcas
  )
#4. Calcular estadísticas básicas de las predicciones
summary(prediccion_EN$.pred)

#5. Calcular el coeficiente de variación de las predicciones
cv_predicciones <- sd(prediccion_EN$.pred) / mean(prediccion_EN$.pred) * 100
print(paste("Coeficiente de variación de las predicciones (%):", round(cv_predicciones, 2)))

#6. Unir predicciones con otras variables

analisis_segmentos <- test_EN %>%
  select(property_type) %>%
  bind_cols(prediccion_EN) %>%
  group_by(property_type) %>%
  summarize(
    media = mean(.pred),
    cv = (sd(.pred) / mean(.pred)) * 100
  )

print(analisis_segmentos)

#7. Identificar posibles valores atípicos
umbral_superior <- quantile(prediccion_EN$.pred, 0.975)
umbral_inferior <- quantile(prediccion_EN$.pred, 0.025)

outliers <- prediccion_EN %>%
  filter(.pred > umbral_superior | .pred < umbral_inferior)

print(paste("Número de outliers en predicciones:", nrow(outliers)))

```

### 2.1.3 Template Kaggle

```{r}
# Descargar archivo
write.csv(resultado_EN_1,
          "C:/../03_Stores/10-04_G4_EN1.csv", row.names = FALSE, 
          quote = FALSE)

write.csv(pred_EN_2,
          "C:/../03_Stores/10-04_G4_EN2.csv", row.names = FALSE, 
          quote = FALSE)

write.csv(pred_EN3,
          "C:/../03_Stores/10-04_G4_EN3.csv", row.names = FALSE, 
          quote = FALSE)
```

## 2.2 CARTs

### 2.2.1 Entrenamiento

```{r}

```

### 2.2.2 Predicción

```{r}

```

### 2.2.3 Template Kaggle

```{r}
# Descargar archivo
write.csv(pred_CARTS1,
          "C:/../03_Stores/10-04_G4_CARTS1.csv", row.names = FALSE, 
          quote = FALSE)

write.csv(pred_CARTS2,
          "C:/../03_Stores/10-04_G4_CARTS2.csv", row.names = FALSE, 
          quote = FALSE)

write.csv(pred_CARTS3,
          "C:/../03_Stores/10-04_G4_CARTS3.csv", row.names = FALSE, 
          quote = FALSE)
```

## 2.3 Random Forest

### 2.3.1 Entrenamiento

```{r}
fitControl <- trainControl(method = "oob")

tree_grid <- expand.grid(
  mtry = 5,
  splitrule = "variance",
  min.node.size = c(2, 3, 4)
)

# Entrenar el modelo con toda la base
model_forest <- train(
  formula_modelo,
  data = train_factors,
  method = "ranger",
  trControl = fitControl,
  tuneGrid = tree_grid,
  num.trees = 500,
  importance = "permutation"
)

# Ver el mejor modelo y métricas OOB
print(model_forest)

# Importancia de variables
varImp(model_forest)
```

### 2.3.2 Predicción

```{r}

```

### 2.3.3 Template Kaggle

```{r}
# Descargar archivo
write.csv(pred_RF1,
          "C:/../03_Stores/10-04_G4_RANDOM-FOREST1.csv", row.names = FALSE, 
          quote = FALSE)

write.csv(pred_RF2,
          "C:/../03_Stores/10-04_G4_RANDOM-FOREST2.csv", row.names = FALSE, 
          quote = FALSE)

write.csv(pred_RF3,
          "C:/../03_Stores/10-04_G4_RANDOM-FOREST3.csv", row.names = FALSE, 
          quote = FALSE)
```

## 2.4 Boosting

### 2.4.1 Entrenamiento

```{r}

```

### 2.4.2 Predicción

```{r}

```

### 2.4.3 Template Kaggle

```{r}
# Descargar archivo
write.csv(pred_boosting1,
          "C:/../03_Stores/10-04_G4_BOOSTING1.csv", row.names = FALSE, 
          quote = FALSE)

write.csv(pred_boosting2,
          "C:/../03_Stores/10-04_G4_BOOSTING2.csv", row.names = FALSE, 
          quote = FALSE)

write.csv(pred_boosting3,
          "C:/../03_Stores/10-04_G4_BOOSTING3.csv", row.names = FALSE, 
          quote = FALSE)
```

## 2.5 Redes neuronales

### 2.5.1 Entrenamiento

```{r}

```

### 2.5.2 Predicción

```{r}

```

### 2.5.3 Template Kaggle

```{r}
# Descargar archivo
write.csv(pred_rneuronales1,
          "C:/../03_Stores/10-04_G4_REDES-NEURONALES1.csv", row.names = FALSE, 
          quote = FALSE)

write.csv(pred_rneuronales2,
          "C:/../03_Stores/10-04_G4_REDES-NEURONALES2.csv", row.names = FALSE, 
          quote = FALSE)

write.csv(pred_rneuronales3,
          "C:/../03_Stores/10-04_G4_REDES-NEURONALES3.csv", row.names = FALSE, 
          quote = FALSE)
```

## 2.6 Super Learners

```{r}

```

## 2.7 Regresión lineal

```{r}

```
